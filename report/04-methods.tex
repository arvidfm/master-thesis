% Copyright (C) 2016  Arvid Fahlstr√∂m Myrman
%
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License as published by
% the Free Software Foundation; either version 2 of the License, or
% (at your option) any later version.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
%
% You should have received a copy of the GNU General Public License along
% with this program; if not, write to the Free Software Foundation, Inc.,
% 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

\chapter{Method}
\label{ch:methods}

This \namecref{ch:methods} describes the shallow siamese network used to find a mapping from input posteriorgrams of high dimensionality to output posteriorgrams of lower dimensionality.
The model is trained using same-class and different-class example pairs, with the goal of minimising (maximising) the Jensen-Shannon divergence between same-class (different-class) pairs.
By enforcing low entropy in the output we encourage sparsity in the model, resulting in a function that approximately maps each input class to a unique output class.

\section{Model}
Given two input vectors $\mat x$ and $\mat y$, we wish to determine whether the vectors belong to the same category, or class.
In the context of speech learning, these vectors might represent two different speech frames, and we wish to find whether they correspond to the same underlying phoneme (speech sound) or not.
To do so, we project the vectors onto an embedding space designed so that vectors belonging to the same class are close, while vectors belonging to different classes are distant.
We consider in particular posteriorgrams---probability vectors $\mat p = (p_1, \dots, p_M)$ representing a posterior distribution over $M$ discrete values.
The probabilities in $\mat p$ sum to $1$, with $p_i$ being the probability of the $i$th value; for instance, it could be the probability that a data point belongs to the $i$th latent class as inferred by a Gaussian mixture model.

We take as input a set $\{(\mat x_i, \mat y_i)\}_{i=1}^N$ of $N$ pairs of $M$-dimensional posteriorgrams.
We assume that the $M$ input probabilities correspond to ``pseudo''-classes (e.g.\ allophones---variants of the same underlying phoneme), where several pseudo-classes together describe a single ``true class'' (e.g.\ phonemes).
Our goal is then to find a function $f$ that maps the $M$ pseudo-classes to a smaller set of $D$ output classes, where we take the probability of a single output class to be the sum of the probabilities of the pseudo-classes that map to the output class in question.
This can also be thought of as finding a partition of the input classes.
It is vital that no two underlying classes are represented by the same input class, as our mapping would not be able to separate the classes.

An important characteristic of the mapping function is that input classes corresponding to the same true class need to be mapped to the same output class, while input classes corresponding to different underlying classes should be mapped to different output classes.
To help us achieve this, we in addition to the input data have a set of indicators $\{c_i\}_{i=1}^N$ such that $c_i$ is $1$ if $\mat x_i$ and $\mat y_i$ belong to the same category, and $0$ otherwise.
We can then restate our goal as finding a mapping where $\mat x_i$ and $\mat y_i$ are close in output space if $c_i = 1$, and distant if $c_i = 0$.
Note that this is different from fully supervised learning, where we know the true class corresponding to each input, since we for each pair only know that the corresponding inputs belong to the same class---we do not know what this class actually is.

To simplify optimisation we relax the problem to one of finding a linear transformation $f : [0,1]^M \to [0,1]^D$ from the original space to a lower-dimensional space.
We consider each output probability to be a weighted combination of input probabilities: $f(\mat x)_j = \sum_{i=1}^M x_i w_{ij}$, or in matrix notation:
\begin{equation}
 f(\mat x) = \mat x \mat W
\end{equation}
where $\mat W = (w_{ij}) \in \mathbb R^{M \times D}$.
If the elements of $\mat W$ are constrained to only take on values in $\{0, 1\}$, and each row of $\mat W$ contains exactly one element with the value $1$, the problem is reduced to finding an exact mapping from input classes to output classes.

Even in the relaxed version of the problem, we need to put certain constraints on $\mat W$ in order to ensure that the output $f(\mat x)$ is a posteriorgram.
First, we need to ensure that all outputs are positive.
As the input $\mat x$ is a posteriorgram, meaning that all elements in $\mat x$ are positive, it clearly suffices to ensure that all elements in $\mat W$ are positive.
Second, the output probabilities must sum to 1.
This can be achieved by ensuring that the elements of each row of $\mat W$ sum to 1, as can be seen by:
\begin{equation}
 f(\mat x) \mat 1_D = \mat x \mat W \mat 1_D = \mat x \mat 1_D = 1
\end{equation}
where $\mat 1_D$ is a column vector of $D$ ones.

In order to ensure that these constraints hold, we construct our model as follows:
\begin{align}
  \mat V &\in \mathbb R^{M \times D} \\
  \mat{\widetilde W} &= |\mat V| \\
  \mat W &= \mat{\widetilde W} \oslash \left(\mat{\widetilde W} \mat 1_D \mat 1_D^T\right) \label{eq:normalize} \\
  f(\mat x) &= \mat x \mat W
\end{align}
where $\mat 1_D^T$ is a row vector of $D$ ones, $|\cdot|$ denotes the element-wise absolute value, and $\oslash$ denotes element-wise division.
Note that the function of \cref{eq:normalize} is to normalise the rows to sum to one.
This formulation makes it possible to optimise the model while ensuring that the constraints on $\mat W$ hold, by performing gradient descent with respect to $\mat V$.
Note that the absolute value is almost everywhere differentiable, and the non-differentiability at $0$ does not matter in practice when optimising using gradient descent.

To encourage the model to place points belonging to the same class close together in the output space, we consider the model as a siamese network.
Conceptually this involves duplicating the model, creating two identical copies of the same network, with the parameters shared.
We then feed one input each to both copies, and calculate the loss function using the corresponding outputs:
\begin{equation}
  L_{\mathrm D}(\mat V; \mat x, \mat y, c) = \begin{cases}D_{\mathrm{same}}(f(\mat x; \mat V), f(\mat y; \mat V)) & \text{if } c = 1 \\
    D_{\mathrm{diff}}(f(\mat x; \mat V), f(\mat y; \mat V)) & \text{if } c = 0\end{cases}
\end{equation}
where $D_{\mathrm{same}}$ and $D_{\mathrm{diff}}$ are the dissimilarity/similarity measures for pairs belonging to the same class, and pairs belonging to different classes, respectively.
The loss function over a minibatch $B$ is given by the average
\begin{equation} \label{eq:nonrebalanced}
  L_{\mathrm D}(\mat V; B) = \frac{1}{|B|} \sum_{i \in B} L(\mat V; \mat x_i, \mat y_i, c_i)
\end{equation}
which is minimised with respect to $\mat V$.

\section{Divergence measure}

As the output of the model is a probability distribution, it makes intuitive sense to use a statistical divergence as a measure of similarity.
Perhaps the most well-known divergence is the Kullback-Leibler (KL) divergence, defined as:
\begin{equation}
  \mathrm{KL}(\mat x || \mat y) = \sum_i x_i \log_2\frac{x_i}{y_i},
\end{equation}
where we take $0 \log_2 0$ to be $0$.
The KL divergence is always positive, and is 0 only if $\mat x = \mat y$.
However, it is asymmetric, unbounded, and undefined if there is an $i$ such that $y_i = 0$ but $x_i \ne 0$.
As such, trying to maximise the dissimilarity between two distributions with respect to the KL divergence is an ill-posed problem, as this will force the divergence to tend towards infinity.

A better choice is the Jensen-Shannon (JS) divergence, defined as
\begin{equation}
  \mathrm{JS}(\mat x || \mat y) = \frac{1}{2} \mathrm{KL}(\mat x || \mat m) + \frac{1}{2} \mathrm{KL}(\mat y || \mat m)
\end{equation}
where $\mat m = (\mat x + \mat y)/2$.
The JS divergence is symmetric, always defined, and is bounded between $0$ (for identical distributions) and $1$ (for distributions with disjoint support), assuming that the base 2 logarithm is used.
Additionally, the square root of the JS divergence is a metric satisfying the triangle inequality \parencite{endres2003new}; here we make use of this fact, in the hope that the metric properties will result in a more well-behaved loss function.

Thus, we define
\begin{align} \label{eq:js-loss}
  D_{\mathrm{same}}(\mat x, \mat y) &= \sqrt{\mathrm{JS}(\mat x || \mat y)} \\
  D_{\mathrm{diff}}(\mat x, \mat y) &= 1 - \sqrt{\mathrm{JS}(\mat x || \mat y)}
\end{align}
thereby minimising the root JS divergence between pairs belonging to the same class, and maximising the divergence between pairs belonging to different classes\footnote{For identical or near-identical $\mat x$ and $\mat y$, the JS divergence may become negative due to rounding errors caused by limited floating point precision; this can be counteracted by adding a small constant value before taking the square root.}.

\section{Balancing same-class and different-class losses}
The above definition of the loss function is sensitive to the ratio of same-class to different-class pairs within a minibatch.
To study the effect of giving different weight to the same-class and different-class losses, we consider the following alternative loss function.
Let $B_1 = \{i \in B : c_i = 1\}$ be the subset of same-class pairs in the current minibatch, and $B_0 = \{i \in B : c_i = 0\}$ the subset of different-class pairs.
We then restate the loss as
\begin{equation}
  \label{eq:rebalanced}
  L_{\mathrm D}(\mat V; B) = \frac{1}{(\alpha + 1)|B_1|} \sum_{i \in B_1} D_{\mathrm{same}}(\hat{\mat x}_i, \hat {\mat y}_i) + \frac{\alpha}{(\alpha + 1)|B_0|} \sum_{i \in B_0} D_{\mathrm{diff}}(\hat {\mat x}_i, \hat {\mat y}_i),
\end{equation}
where $\hat{\mat x}_i = f(\mat x_i; \mat V)$ and $\hat{\mat y}_i = f(\mat y_i; \mat V)$.
$\alpha$ is a hyperparameter specifying how much more to weight the different-class loss over the same-class loss.
A small $\alpha$ will emphasise the same-class loss, bringing the output vectors closer in output space, while a large $\alpha$ will cause the output vectors to be placed further apart on average.

\section{Entropy penalty}

To make the output of the model interpretable, it is desirable to ensure that for a given input, only one output unit is active.
This can be done by introducing an entropy penalty, which attempts to minimise the spread of the probability mass.
The entropy of a probability vector $\mat x = (x_1, \dots, x_D)$ is defined as
\begin{equation}
  H(\mat x) = -\sum_{i=1}^D x_i \log_2 x_i.
\end{equation}
However, this definition is sensitive to the value of $D$; for instance, the entropy of a uniform distribution vector is $\log_2 D$.

As we may wish to vary the number of outputs of the model, it is of interest for the entropy penalty to be invariant to $D$.
We therefore introduce the normalised entropy, defined as
\begin{equation}
  \hat H(\mat x) = \frac{1}{\log_2 D} H(\mat x).
\end{equation}
The normalised entropy is always between 0 (for degenerate distributions) and 1 (for uniform distributions).
Over a minibatch $B$, we define the entropy loss as
\begin{equation}
  L_{\mathrm{H}}(\mat V; B) = \frac{1}{2|B|} \sum_{i \in B} \left(\hat H(f(\mat x_i; \mat V)) + \hat H(f(\mat y_i; \mat V))\right).
\end{equation}

The entropy penalty implicitly encourages sparsity in $\mat W$, as the only way to avoid spreading the probability mass across several outputs is for each row of $\mat W$ to only contain a single element close to $1$.
It is thus through this penalty that we enforce the model to find an approximate mapping of input to output classes.
This also illustrates that $\alpha$ needs to be carefully tuned in \cref{eq:rebalanced}: A too small $\alpha$ will cause too many input classes to merge, including input classes that correspond to completely different underlying classes, while a too large $\alpha$ will cause input classes that do correspond to the same underlying class to fail to merge.

In summary, our final loss function over a minibatch $B$ is as follows:
\begin{equation}
  \label{eq:complete-loss}
  L(\mat V; B) = L_{\mathrm{D}}(\mat V; B) + \lambda L_{\mathrm{H}}(\mat V; B)
\end{equation}
where $\lambda$ is a hyperparameter.

\subsection{Binarising the model}
\label{sec:discrete}
As the resulting model is sparse, we can construct an exact partition of the input classes.
We do this by setting the largest element in each row in $\mat W$ to $1$, and the remaining elements to $0$, resulting in a binary $\mat W$.
An optional further processing step is to binarise the output distribution by setting the largest output to 1 and the rest to 0; this can be thought of as taking the argmax of the output distribution, which corresponds to the crosses shown in \cref{fig:model-output}.

\subsection{Counting the number of outputs}
\label{sec:spread}

As the model is sparse, it may choose to only make use of a subset of the available outputs.
As a heuristic estimate of the number of outputs used by the model, we define the ``spread'' of the model as follows.
We take the average of the $j$th column of the normalised weight matrix:
\begin{equation}
  q_j = \frac{1}{M} \sum_{i=1}^M w_{ij}.
\end{equation}
This represents the average mapping to the $i$th output.
As $\mat W$ is row-normalised, the elements of $Q = (q_1, q_2, \dots, q_D)$ sum to 1, and we can thus treat $Q$ as describing a probability distribution.
A uniform distribution can be thought of as meaning that each output has the same number of inputs mapped to it, assuming a perfectly sparse weight matrix.
Now consider the case where $K$ of the $M$ outputs receive the same number of inputs, with no inputs mapping to any of the remaining $M - K$ outputs (i.e.\ only $K$ outputs are used by the model).
The entropy is then given by
\begin{equation}
  H(Q) = - \sum_{i=1}^K \frac{1}{K} \log_2 \frac{1}{K} = \log_2 K.
\end{equation}
Solving for $K$ we have
\begin{equation}
  K = 2^{H(Q)},
\end{equation}
which can be used in general an approximation of the number of outputs used by the model, which we denote as the spread.
This can, for instance, be used to gauge the behaviour of the model as $\alpha$ is varied.
A value of $K$ close to $D$ is an indicator that all the outputs are being used by the model, suggesting that it may be a good idea to increase the number of available outputs.

\section{Evaluation}

We evaluate the models on the minimal-pair ABX task \parencite{schatz2013evaluating}.
In the task we are presented with three speech fragments A, B and X, where A and B form minimal pairs, i.e.\ they only differ by a single phoneme.
The task is to decide which of either A or B belongs to the same category as X.
This is done by aligning A and B with X using DTW with respect to some underlying frame-based metric.
The fragment closest to X according to the DTW score is chosen.
The task takes two forms: within-speaker discriminability, where all fragments belong to the same speaker, and across-speaker discriminability, where A and B belong to one speaker while X belongs to another.
The final score is the percentage of triples for which the wrong fragment was chosen as being of the same category as X.
