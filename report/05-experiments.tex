% Copyright (C) 2016  Arvid Fahlström Myrman
%
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License as published by
% the Free Software Foundation; either version 2 of the License, or
% (at your option) any later version.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
%
% You should have received a copy of the GNU General Public License along
% with this program; if not, write to the Free Software Foundation, Inc.,
% 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

\chapter{Experiments}
\label{ch:experiments}

This chapter describes the application of the model described in \cref{ch:methods} to the task of unsupervised modelling of speech, and in particular the use of the model to improve posteriorgrams generated from a Gaussian mixture model.
First the experimental setup is described, including the data used, the how the data is processed, and how the models are implemented.
Next, a number of experiments aimed at tuning hyperparameters are described.
Finally, we evaluate a number of models using the minimal-pair ABX task.

\section{Data}
%\subsection{Data}
We evaluate our method using the data used by the 2015 Zero Resource Speech Challenge in the context of unsupervised learning of speech units.
The challenge makes use of two corpora: The Buckeye corpus of conersational English \parencite{buckeyecorpus} and the NCHLT speech corpus of read Xitsonga \parencite{barnard2014nchlt}.
For the challenge only a subset of the data is used, consisting of 12 speakers for a total of 5 hours of data for the Buckeye corpus, and 24 speakers for a total of 2.5 hours of data for the NCHLT Xitsonga corpus.
Additionally provided is voice activity information indicating segments containing clean speech, as well as labels indicating the identity of the speaker.

%\subsection{Generating the posteriorgrams (to experimental setup/method)}
%\label{sec:posteriorgrams}
MFCC features were extracted from the data using a frame window length of \SI{25}{\ms} which was shifted \SI{10}{ms} for each frame, an FFT resolution of 512 frequency steps, and 40 mel-spaced triangular filter banks.
13 coefficients with both delta and delta-delta features were used.
The MFCCs corresponding to segments with voice activity were clustered using an implementation of a Gaussian mixture model (GMM) provided by scikit-learn \parencite{scikit-learn}.
The GMM was trained using the expectation maximisation algorithm, using $M = 1024$ Gaussians with diagonal covariance matrices, for a maximum of 200 iterations.
Once training has finished the posteriorgram for the $n$th frame is constructed as $\mat p_n = (p_n^1, p_n^2, \dots, p_n^{1024})$ where $p_n^k = p(z_n = k \mid \mat x_n)$ is the posterior probability of the $k$th class given the $n$th frame.

The unsupervised term discovery yielded 6512 fragments and 3149 clusters for the Buckeye corpus, and 3582 fragments and 1782 clusters for the NCHLT Xitsonga corpus\footnote{The cluster files used for this work were generously provided by Roland Thiollière and Aren Jansen.}.
70\% of the same-class and different-class fragment pairs were used for training, with the remaining pairs used for validation to determine when to interrupt the training of the models.

\section{Experimental setup}

\subsection{Models}
\label{sec:models}

We compare the following four shallow models:
\begin{itemize}
  \item A model trained with the non-rebalanced loss of \cref{eq:nonrebalanced}.
  \item A model trained with the rebalanced loss of \cref{eq:rebalanced}, to measure the influence of reweighting the losses.
  \item The rebalanced model, where we binarise the weight matrix $\mat W$ after training to produce an exact partition.
  \item The model with binary $\mat W$, where we further binarise the output.
\end{itemize}
All shallow models use $D = 64$ outputs.
The values chosen for the $\lambda$ and $\alpha$ hyperparameters are described in \cref{sec:tune-entropy,sec:rebalanced-exp}.

To compare to the shallow models, we also train the following two deep models:
\begin{itemize}
 \item A network with $64$ softmax outputs, trained using the non-rebalanced JS loss, to get an idea of how the JS loss performs in general.
 \item A network with $100$ sigmoid outputs, trained using the coscos$^2$ loss of \textcite{synnaeve2014phonetics}.
\end{itemize}
Both deep models use two hidden layers of $500$ sigmoid units each.
The latter model corresponds to the architecture used by \textcite{thiolliere2015hybrid}.
Note that we do not enforce any entropy penalty for the deep network trained with the JS loss, as the softmax outputs are naturally sparse.
Additionally, no penalty is used when training using the coscos$^2$ loss.
In place of posteriorgrams we use log-scale outputs of 40 mel-scaled filter banks, normalised to have zero mean and unit variance over the whole data set and with a context of 3 frames on both sides, for a total of 280 values as input to the deep networks.

All models were trained using AdaMax \parencite{kingma2014adam} with the recommended default parameters $\alpha = 0.002$, $\beta_1 = 0.9$ and $\beta_2 = 0.999$.
All frames used for training were shuffled once at the start of training, and a minibatch size of 1000 frames was used.
The models were trained until no improvement had been observed on a held-out validation set for 15 epochs, where an epoch is defined as one use of the whole training data.

All network models were implemented in Python 3.5 using Theano \parencite{theano} for automatic differentiation and GPU acceleration, librosa \parencite{librosa} for feature extraction, scikit-learn \parencite{scikit-learn} for various utilities, and numba \parencite{numba} for accelerating various code, in particular dynamic time warping.
The training was performed on a GeForce GTX TITAN\footnote{The GeForce GTX TITAN used in this research was donated by the NVIDIA Corporation.} with 6~GB VRAM and 12 Intel i7-5930K cores clocked at 3.50~GHz, with 64~GB RAM.

\subsection{Entropy penalty hyperparameter}
\label{sec:tune-entropy}

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{groupplot}[group style={group size=2 by 1, horizontal sep=2cm},
      ymin=0,ymax=0.7,enlarge x limits=true,width=5cm,height=4cm]
      \nextgroupplot[title=English,xlabel=Penalty ($\lambda$),ylabel=Divergence/entropy,
      legend style={column sep=10pt},legend entries={JS loss,Same-class loss,Different-class loss,Normalised entropy},
      legend columns=2,legend to name=grouplegend,legend cell align=left]
   \addplot table[x=lambda,y=js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=same-js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=diff-js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=entropy-v] {data/entropy_buckeye.txt};
   
   \nextgroupplot[title=Xitsonga,xlabel=Penalty ($\lambda$),ylabel=Divergence/entropy]
   \addplot table[x=lambda,y=js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=same-js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=diff-js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=entropy-v] {data/entropy_xitsonga.txt};
  \end{groupplot}
  \node[yshift=1.6cm] at ($(group c1r1.north)!.5!(group c2r1.north)$) {\ref{grouplegend}};
\end{tikzpicture}

\caption{\label{fig:entropy-penalty} Effect of varying the entropy penalty hyperparameter for the English (left) and Xitsonga (right) data sets.
The average entropy of the output distribution over the validation samples is shown along with the (root) Jensen-Shannon loss: Both the combined JS loss that is optimised for, and separately for same-class and different-class frame pairs.}
\end{figure}

The entropy penalty hyperparameter $\lambda$ is a free parameter, which is data dependent and must be manually specified.
Ideally, $\lambda$ should be the smallest value for which the entropy loss is minimised, or nearly minimised, to avoid sacrificing the Jensen-Shannon loss.
We train models using $\lambda \in \{0, 0.05, 0.1, \dots, 0.3\}$ for both the English and Xitsonga data sets.
The final validation errors for each model are reported in \cref{fig:entropy-penalty}.
For both data sets, the entropy drops quickly even for small $\lambda$, suggesting that the entropy is relatively easy to optimise for.
As $\lambda$ is increased beyond $0.1$, the entropy loss itself does not decrease significantly; however, the different-class JS loss decreases at the expense of the same-class JS loss.
For evaluation, we use $\lambda = 0.1$ to train our models, as little improvement of the entropy is seen beyond this value for either data set.

\subsection{Rebalanced loss function hyperparameter}
\label{sec:rebalanced-exp}

\begin{figure}
 \centering
 \begin{tikzpicture}
   \pgfplotsset{set layers}
   \begin{axis}[
     scale only axis,
     xmin=0.8,xmax=4.2,
     ymax=0.25,
     axis y line*=left,
     xlabel=$\alpha$,
     ylabel=Silhouette,
     yticklabel style={/pgf/number format/fixed},
     height=5cm,width=8cm,
     legend style={opacity=0.0}]%,
      %legend style={column sep=10pt},legend entries={Silhouette (English),Silhouette (Xitsonga)},legend cell align=left]
      \addplot table[x=alpha,y=sil-en] {data/silhouette.txt}; \label{sil1}
      \addlegendentry{Silhouette (English)}
   %\addplot+[mark=x] table[x=alpha,y=spread-en] {data/silhouette.txt};
   \addplot table[x=alpha,y=sil-ts] {data/silhouette.txt}; \label{sil2}
   \addlegendentry{Silhouette (Xitsonga)}
   %\addplot+[mark=x] table[x=alpha,y=spread-ts] {data/silhouette.txt};
   \end{axis}
   \begin{axis}[
     scale only axis,
     xmin=0.8,xmax=4.2,
     ymax=80,
     axis y line*=right,
     axis x line=none,
     ylabel=Spread,
     height=5cm,width=8cm,
     legend style={anchor=north west,at={(0.02,0.98)}}]%,
      %legend style={column sep=10pt},legend entries={Silhouette (English),Silhouette (Xitsonga)},legend cell align=left]
   %\addplot+[mark=x] table[x=alpha,y=sil-en] {data/silhouette.txt};
   \addlegendimage{/pgfplots/refstyle=sil1}\addlegendentry{Silhouette (English)}
   \addlegendimage{/pgfplots/refstyle=sil2}\addlegendentry{Silhouette (Xitsonga)}
   \addplot+[dashed] table[x=alpha,y=spread-en] {data/silhouette.txt};
   \addlegendentry{Spread (English)}
   %\addplot+[mark=x] table[x=alpha,y=sil-ts] {data/silhouette.txt};
   \addplot+[dashed] table[x=alpha,y=spread-ts] {data/silhouette.txt};
   \addlegendentry{Spread (Xitsonga)}
   \end{axis}
 \end{tikzpicture}

 \caption{\label{fig:silhouette} Silhouette and spread for different weightings of the same-class and different-class losses.}
\end{figure}

We use the silhouette to find the best setting of $\alpha$.
Models were trained for $\alpha \in \{1, 1.5, 2, 2.5, 3, 3.5, 4\}$, where we fix the entropy penalty hyperparameter to $\lambda = 0.1$.
The silhouette is calculated on a subset of 1000 of the fragment clusters, using the output of the trained models to represent the frames of the fragments.
\Cref{fig:silhouette} shows the silhouette, as well as the spread as defined in \cref{sec:spread}, for different values of $\alpha$.
As one might expect, more emphasis on the different-class loss results in a higher spread, i.e.\ a larger number of output classes.
The optimal value of $\alpha$ is found to be 1.5 for both data sets, and we use this value when training the models to evaluate.

\section{Results}

\subsection{Overfitting}
\begin{figure}
 \centering
 \begin{tikzpicture}
  \begin{groupplot}[group style={group size=2 by 2, horizontal sep=2cm,vertical sep=2.2cm},
    ymin=0,ymax=0.7,enlarge x limits=true,width=5cm,height=4cm,xlabel=Epoch,ylabel=Loss,no markers]
    \nextgroupplot[title=Shallow JS (English),legend entries={Training error,Validation error},
      legend to name=overfitlegend,legend columns=2,ymin=0.1,ymax=0.6]
    \addplot table[x=epoch,y=train] {data/errors-rownorm-english.txt};
    \addplot table[x=epoch,y=valid] {data/errors-rownorm-english.txt};
    \nextgroupplot[title=Shallow JS (Xitsonga),ymin=0.1,ymax=0.6]
    \addplot table[x=epoch,y=train] {data/errors-rownorm-xitsonga.txt};
    \addplot table[x=epoch,y=valid] {data/errors-rownorm-xitsonga.txt};
    \nextgroupplot[title=Deep coscos$^2$ (English),ymin=0,ymax=0.5]
    \addplot table[x=epoch,y=train] {data/errors-deep-english.txt};
    \addplot table[x=epoch,y=valid] {data/errors-deep-english.txt};
    \nextgroupplot[title=Deep coscos$^2$ (Xitsonga),ymin=0,ymax=0.5]
    \addplot table[x=epoch,y=train] {data/errors-deep-xitsonga.txt};
    \addplot table[x=epoch,y=valid] {data/errors-deep-xitsonga.txt};
   
   %\nextgroupplot[title=Xitsonga,xlabel=Penalty ($\lambda$),ylabel=Divergence/entropy]
   %\addplot table[x=lambda,y=js-v] {data/entropy_xitsonga.txt};
   %\addplot table[x=lambda,y=same-js-v] {data/entropy_xitsonga.txt};
   %\addplot table[x=lambda,y=diff-js-v] {data/entropy_xitsonga.txt};
   %\addplot table[x=lambda,y=entropy-v] {data/entropy_xitsonga.txt};
  \end{groupplot}
  \node[yshift=1.6cm] at ($(group c1r1.north)!.5!(group c2r1.north)$) {\ref{overfitlegend}};
 \end{tikzpicture}
 
 \caption{\label{fig:overfitting}Comparison of training and validation errors for the shallow JS loss-based and the deep coscos$^2$-based models.}
\end{figure}

\Cref{fig:overfitting} shows the training and validation errors from two of the models: The proposed model with the rebalanced JS loss, and the deep model trained with the coscos$^2$ loss.
We can see that the training and validation errors clearly decrease at different rates for the deep models, while the validation error more closely follows the training error for the shallow models.
This indicates that the shallow model is indeed more robust to overfitting.
This in turn makes it possible to train the shallow models for longer without causing the validation error to increase.

\subsection{Model sparsity}

We find that the shallow models are indeed sparse after training when enforcing the entropy penalty.
For instance, after training the model with $\lambda = 0.1$ using the non-rebalanced JS loss, we find that the largest element on each row of $\mat W$ is on average $0.98$ for English and $0.92$ for Xitsonga across the 1024 rows.
We can inspect $\mat W$ to see how many of the $64$ outputs are actually being used by the model.
We take the sum over each column of $\mat W$, which roughly corresponds to how many inputs are mapped to each output.
We find that for both English and Xitsonga, this sum is above $0.5$ for only a minority of outputs: 11 outputs for English, and 10 outputs for Xitsonga.
For English, where $\mat W$ is particularly sparse, none of the other 53 sums even reach $0.05$.
As only 10 or 11 output classes is unrealistic in terms of the number of phonemes present in the languages, this confirms the need to reweight the same-class and different-class losses.

\subsection{ABX scores}

\begin{sidewaystable}
 \centering
 \begin{tabular}{llrrrrrr} \toprule
   && \multicolumn{3}{c}{English} & \multicolumn{3}{c}{Xitsonga} \\ \cmidrule(lr){3-5} \cmidrule(lr){6-8}
    & Model & Silhouette & Within & Across & Silhouette & Within & Across \\ \midrule
    Baseline & GMM posteriors & 0.008 & 12.3 & 23.8 & 0.066 & 11.4 & 23.2 \\ \midrule
    Proposed & %Real $\mat W$ ($\alpha = 1$) & 0.093 & 14.1 & 21.2 & 0.119 & 15.8 & 25.1 \\
    Non-rebalanced & 0.089 & 14.2 & 21.4 & 0.111 & 16.5 & 25.6 \\
    models & Rebalanced ($\alpha = 1.5$) & 0.108 & 12.8 & 19.8 & 0.146 & 14.0 & 23.2 \\
    & Binary $\mat W$ & 0.124 & 12.0 & 19.3 & 0.170 & 12.7 & 21.9 \\
    & Binary output & 0.010 & 16.5 & 24.6 & 0.014 & 19.4 & 29.2 \\ \midrule
    Related & %Deep JS & -0.058 & 18.1 & 25.6 & 0.150 & 17.5 & 23.5 \\
    Deep JS & -0.370 & 22.4 & 28.2 & -0.320 & 18.2 & 24.8 \\
    models & Deep coscos$^2$ (own implementation) & 0.175 & 12.0 & 19.7 & 0.298 & 11.8 & 19.2 \\
    & Deep coscos$^2$ \parencite{thiolliere2015hybrid} & --- & 12.0 & 17.9 & --- & 11.7 & 16.6 \\
    & DPGMM + LDA \parencite{heck2016unsupervised} & --- & 10.6 & 16.0 & --- & 8.0 & 12.6 \\ \bottomrule
 \end{tabular}

 \caption{\label{tab:abx}ABX and silhouette results for the models described in \cref{sec:models}.}
\end{sidewaystable}

We use the ABX toolkit provided for the Zero Resource Speech Challenge \parencite{versteegh2015zero} for evaluating the models.
The results of the evaluation are shown in \cref{tab:abx}, along with the silhouette for each model.
The silhouette is calculated as in \cref{sec:tuning-rebalanced,sec:rebalanced-exp}; higher is better.
The ABX scores are shown as the percentage of ABX triples for which the model answered incorrectly; lower is better.
We show results for both the within-speaker and across-speaker ABX tasks.

We can see that in general, the silhouette seems to be indicative of the relative performance of the models on the ABX task, with well-performing models having a higher silhouette score.
Among the shallow models, we see that rebalancing the same-class and different-class losses results in significant gains, with binarisation of the weights further improving the results.
Unsuprisingly however, binarising the output as well severely worsens the results, likely due to too much information being discarded.
We find that while the models perform worse than the current state-of-the-art \parencite{heck2016unsupervised}, especially for Xitsonga, they were generally able to improve on the input posteriorgrams, especially for the across-speaker task.

The resulting shallow models are very sparse, with the average row-wise maximum weight of $\mat W$ being 0.991 for English and 0.929 for Xitsonga, for ${\alpha = 1.5}$.
This also results in only a subset of the available outputs being used, with 33 outputs receiving any probability mass when binarising the English model; for Xitsonga 35 outputs were used.

The deep model performs poorly when trained with the Jensen-Shannon loss, despite a similar architecture performing well when trained with the coscos$^2$ loss.
Inspecting the average output of the deep model over the English data set, we found that only 6 outputs are actually used by the model.
This suggests that the JS loss is more sensitive than the coscos$^2$ loss when it comes to balancing the same-class and different-class losses.
Note that we were unable to replicate the results of \textcite{thiolliere2015hybrid} using the coscos$^2$ loss.
