% Copyright (C) 2016  Arvid Fahlstr√∂m Myrman
%
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License as published by
% the Free Software Foundation; either version 2 of the License, or
% (at your option) any later version.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
%
% You should have received a copy of the GNU General Public License along
% with this program; if not, write to the Free Software Foundation, Inc.,
% 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

\chapter{Theory}

\section{Audio processing for speech recognition}

An important step in most speech recognition applications is feature extraction: From a raw audio signal we wish to extract information (features) that can be used to effectively process or model the speech to achieve some desired result.
As the exact types of features used vary depending on the model used and the goal of the application, this section will focus on some particularly common features used in speech recognition.
Additionally, some standard methods of processing speech that take into consideration the sequential and dynamic nature of speech will be discussed.

This section serves as a superficial introduction to signal processing for speech recognition.
For a more in-depth description, see a book on signal processing such as \textcite{quatieri2002discrete}, or see \textcite{huang2001spoken} for an introduction to speech recognition in particular.

\subsection{Audio signals}

In the real world speech takes the form of a pressure wave generated as air is pushed through the vocal tract.
The pressure wave as perceived from a single point in space can be described as a continuous signal $x(t)\; (t\in \mathbb{R})$ \todo{notation for domain?} that varies smoothly in time, with $x(t)$ at each time $t$ describing the amplitude of the wave relative to the ambient pressure.
Our perception of the signal depends not on the absolute value of $x(t)$ at any given time instant, but rather on how it varies in time.
As an example, take a simple sine wave $x(t) = \sin(2\pi f t)$; though the signal oscillates continuously in time, a human would perceive the signal as a single constant sound of frequency $f$.

However, while the physical signal is continuous, digital computers are unable to handle signals with an infinitely high time resolution, and as a consequence the signal must be somehow discretised before it can be processed further by speech applications.
This is done by taking samples of the signal at fixed time steps given by a sampling frequency $f_s$ (e.g.\ \SI{16000}{\Hz}), specifying the number of samples taken per second.
The resulting sampled signal $x[n]\; (n \in \mathbb{Z})$ is a discrete-time approximation of the original signal $x(t)$.

\subsection{Short-time Fourier transform}

\todo[inline]{maybe mention window functions?}
\todo[inline]{something about the Nyquist frequency}

To mirror how humans perceive audio signals it is useful to transfer the signal to some form that better captures the fluctuations of the signal.
One way is to analyse the \emph{frequency content} of the signal using the Fourier transform.
The Fourier transform approximates the signal using a sum of sine and cosine waves of different frequencies, giving the amplitude of each such wave, which in turn can be interpreted as the energy content of the signal at the corresponding frequency.
In particular, the discrete Fourier transform (DFT), defined as
\[
X(k) = \sum_{n=0}^{N-1} x[n]e^{-j2\pi kn/N}
\]
where $j$ is the imaginary unit and $k \in [0, N-1]$ corresponds to the frequency $f = \frac{k}{N}f_s$, gives the frequency content of a finite discrete-time signal of length $N$ samples.
The energy density, or the energy distribution over frequency, is given by $S(k) = \left|X(k)\right|^2$.
The DFT can be calculated in $O(N \log N)$ asymptotic time using the fast Fourier transform (FFT) algorithm, especially in the case where $N$ is chosen to be a power of 2 \parencite{cooley1965algorithm}.

\todo[inline]{sources for the paragraph below}

\begin{figure}
  \centering
  \input{figures/stft}
  \caption{\label{fig:sftf}Example of running a short-time Fourier transform on a \SI{25}{\ms} section of a recording of the word ``bed''.
  The dashed line shows the ``envelope'', or overall shape, of the energy spectrum.
  The peaks and valleys of the spectrum are characteristic of sonorant sounds such as vowels.}
\end{figure}

The DFT makes certain assumptions regarding the nature of the signal.
In particular, it assumes that the signal is periodic, which is emphatically not true in general for speech signals.
However, a speech signal can be thought to be \emph{approximately periodic} over a very short time period.
This gives rise to the so-called short-time Fourier transform (STFT), where the DFT is calculated repeatedly on short sections of the signal using a sliding window; see \cref{fig:sftf} for an example.
A typical window length is \SI{25}{\ms}, and it is generally shifted forward about \SI{10}{\ms} between each DFT calculation.
The result of the STFT is the 2D Fourier transform $X(m,k)$ over time step $m$ and frequency $k$, with corresponding energy density $S(m,k) = \left|X(k,m)\right|^2$.

\subsection{Mel-scale filter banks}

There are several problems with working directly with the output of the STFT. One is that the output is too high-dimensional, as $N$ is often chosen to be in the \numrange{512}{2048} range for the DFT.
Unless very large amounts of data is available, this causes data sparsity issues, which can cause many kinds of models to underperform.
Additionally, we are not really interested in the exact energy at every frequency step, but 
rather the overall shape of the spectrum.

\begin{figure}
  \centering
  \input{figures/mel-scale}
  \caption{\label{fig:melscale}The mel scale as it corresponds to the standard frequency scale.
  Higher frequencies are closer together on the mel scale than lower frequencies.}
\end{figure}

\begin{figure}
  \centering
  \input{figures/mel-filterbank}
  \caption{\label{fig:filterbank}20 triangular filters spaced linearly along the mel scale from \SIrange{0}{8000}{\Hz}.
  The peaks of the filters are scaled as to ensure constant area.}
\end{figure}

Finally, all frequencies are not created equal, as the human ear does not discriminate between higher frequencies to the same extent as between lower frequencies.
To model this phonemenon, scales in which a change in pitch corresponds roughly linearly to the subjective change in pitch perceived by a human have been empirically developed.
One such scale, ubiquitous in speech technology, is the mel scale, which was developed through experiments where participants were told to produce a tone with half the perceived pitch of a reference tone \parencite{stevens1937scale}.
A frequency $f$ can be converted to the mel scale through the following relation:
\[
  \mathrm{mel}(f) = 1127\log\left(1 + \frac{f}{700}\right)
\]
where $\log$ is the natural logarithm.
As can be seen in \cref{fig:melscale}, as $f$ grows larger, the difference $\mathrm{mel}(f+\varepsilon) - \mathrm{mel}(f)$ becomes smaller.

Addressing both the issues of dimensionality and perception, we construct the mel-scale filter bank.
The filter bank is a set of $L$ triangular filters, with the middle points of the filters spaced linearly on the mel scale.
In other words, if $f_1, f_2, \dots, f_L$ are the middle points of the filters specified in \si{\Hz}, $\mathrm{mel}(f_k) - \mathrm{mel}(f_{k-1}) = \mathrm{mel}(f_{k+1}) - \mathrm{mel}(f_k)$, $k \in [2, L-1]$.
The start and end points of the filters are the middle points of the previous and following filters, respectively, with the exception of the first filter whose start point is specified by a lower bound $f_{low}$, and the last filter whose end point is given by a higher bound $f_{high}$.
The height of the peak of each filter can vary; common approaches are to ensure that the filters have either constant height or constant area.
An illustration of a filter bank is given in \cref{fig:filterbank}.
Each filter is applied to the energy spectrum of the audio signal, giving the filter bank output $E(m,l) = \sum_{k=0}^{N-1}V_l(k)S(m,k)$, where $V_l(k)$ is the $(l+1)$th mel-scale filter.
The resulting $L$ values at each time step $m$ form a rough approximation of the energy spectrum, with the resolution being higher for low frequencies than for high frequencies.
$L$ is commonly chosen to be in the \numrange{20}{40} range, significantly lowering the dimensionality of the data.

\subsection{Mel frequency cepstral coefficients}

In certain applications, it is beneficial if the features generated are relatively decorrelated.
For instance, this is the case when modelling the features using multivariate Gaussian distributions, where if the features are decorrelated the covariance matrix can be approximated using a diagonal matrix, significantly reducing the number of parameters that need to be learnt.

Unfortunately, the mel-scale filter bank outputs are \emph{not} decorrelated, as neighbouring frequencies tend to take on similar values in the energy spectrum.
However, they can be made roughly decorrelated by taking the discrete cosine transform (DCT) of the logarithm of the filter bank outputs, given as
\[
C(m,c) = \sum_{l=0}^{L-1} \log(E(m,l)) \cos\left[\frac{2\pi}{L}\left(l+\frac{1}{2}\right)c\right]
\]
for $c \in [0, L - 1]$.
The DCT approximates the input using cosine waves in a similar manner to how the Fourier transform works, with each coefficient $c$ indicating how similar the input is to the cosine wave with frequency $c/L$\todo{true?}.

The resulting values are known as the mel-frequency cepstral coefficients (MFCCs).
Usually only the first 13 or so coefficients are used at each time step, i.e.\ the coefficients corresponding to $c \in [0, 12]$, once again reducing the dimensionality of the data.

\subsection{Modelling evolution over time}

Speech is inherently sequential.
This means that our perception of speech is not dependent on particular absolute values of the speech signal at particular points in time, but rather on how the signal evolves over time, and the exact realisation of individual speech sounds is formed through complex interplay with neighbouring sounds.
In addition, many speech sounds change over time by nature; examples include the diphthong /a…™/ in the word \emph{my} /ma…™/, or the affricate /t É/ in \emph{teach} /tiÀêt É/, both acting as single units of speech, despite the onset and offset of the sounds being significantly different acoustically.

Thus, it is a difficult task to recognise a speech sound based only on a single frame of audio.
Instead, the model needs some way of incorporating information about the context of the frame.
This can be done both at the model level and the feature level.
Model-based approaches include hidden Markov models (HMMs), which encode information about how the speech can change in time in the form of probabilities, and recurrent neural networks (RNNs), which can take sequences as input and automatically find temporal patterns.

Feature-based approaches, instead, incorporate temporal information directly into the features.
One simple way is to extend each speech frame to include not only the current frame, but also the neighbouring frames before feeding it to the model.
For instance, if our features consisted of the outputs of a mel-scaled filter bank of size 40 at different time steps $m$ and we wanted to include a context of 2 frames in both directions it time, the frame at each $m$ would be extended to include the frames at $m-2$, $m-1$, $m$, $m+1$ and $m+2$, resulting in a feature vector of size $40\cdot5 = 200$ at each time step.

\todo[inline]{better source for below}

Another feature-based approach is to include approximations of the temporal derivatives in the feature vector, most commonly the first-order (velocity) and second-order (accerelation) derivatives.
We do this by approximating the feature vector sequence using a second-order polynomial $f(k)=a + bk + ck^2$ and taking its derivative $f'(k)=b + 2ck$.
Let $y_{-n}, \dots, y_{-1}, y_0, y_1, \dots, y_n$ be a sequence of feature values (e.g.\ the values corresponding to a single MFCC), where we are interested in the temporal derivative at the point corresponding to $y_0$, i.e.\ at $k=0$.
$n$ is the number of points at each side of $y_0$ that we want to use to estimate the polynomial.
We wish to find the coefficients that minimise
\[
\sum_{k=-n}^n (f(k) - y_k)^2.
\]
As the derivative of $f(k)$ at $k=0$ is $f'(0) = b$, we only need to find a solution for $b$.
We minimise the error function by taking the gradient with respect to $b$ and setting it to $0$:
\begin{align*}
  \frac{\partial}{\partial b} \sum_{k=-n}^n (f(k) - y_k)^2 &= 0 \\
  \frac{\partial}{\partial b} \sum_{k=-n}^n (a + bk + ck^2 - y_k)^2 &= 0 \\
  \sum_{k=-n}^n 2k(a + bk + ck^2 - y_k) &= 0 \\
  \sum_{k=-n}^n ak + \sum_{k=-n}^n bk^2 + \sum_{k=-n}^n ck^3 &= \sum_{k=-n}^n ky_k.
\end{align*}
By antisymmetry we find that $\sum_{k=-n}^n ak = \sum_{k=-n}^n ck^3 = 0$, leaving us with
\begin{align*}
  \sum_{k=-n}^n bk^2 &= \sum_{k=-n}^n ky_k \\
  b &= \frac{\sum_{k=-n}^n ky_k}{\sum_{k=-n}^n k^2} \\
  f'(0) &= \frac{\sum_{k=1}^n k(y_k - y_{-k})}{2\sum_{k=1}^n k^2}.
\end{align*}
Thus, in general, to approximate the first-order temporal derivative at a point in time $t$, also known as the \emph{delta} value at $t$, we have
\[
\Delta y_t \approx \frac{\sum_{k=1}^n k(y_{t+k} - y_{t-k})}{2 \sum_{k=1}^n k^2}
\]
which is the formula used by toolkits such as HTK \parencite{young2005htk}.
The second-order derivative, or the \emph{delta-delta} values, can be obtained by repeating the process using the delta values.

\subsection{Dynamic time warping}

Speech is also inherently dynamic.
A single speaker if asked to repeat a word two times will not pronounce the word exactly the same both times, and the length of the utterances will also differ slightly.
As a result of this, direct comparison of two utterances in order to measure their similarity becomes difficult.
Dynamic time warping (DTW) attempts to solve this problem by finding the best alignment of the frames of the two utterances and measure the similarity based on this alignment.

Let $d(\mat x, \mat y)$ be some local measure of the distance between the feature vectors $\mat x$ and $\mat y$ (e.g.\ the Euclidean distance) so that $d(\mat x, \mat y)$ is larger the further apart the feature vectors are.
Let $\mat X = \{\mat x_1, \mat x_2, \dots, \mat x_n\}$ and $\mat Y = \{\mat y_1, \mat y_2, \dots, \mat y_m\}$ be sequences of feature vectors, and let $\mat X_{k:l}$ denote the subsequence of $\mat X$ starting at $k$ and ending at $l$. 
We wish to find a global distance measure $D(\mat X, \mat Y)$ as a sum of the local distances between the feature vectors, based on some alignment that minimises this distance.
All elements in both sequences must be used in the final alignment, but elements may be repeated at will to serve as padding.

This can be expressed through the recurrence
\begin{align*}
D(\mat X_{1:k}, \mat Y_{1:l}) = d(\mat x_k, \mat y_l) + \min(&D(\mat X_{1:k-1}, \mat Y_{1:l-1}), \\
& D(\mat X_{1:k-1}, \mat Y_{1:l}), \\
& D(\mat X_{1:k}, \mat Y_{1:l-1}))
\end{align*}
with the base cases
\begin{align*}
  D(\mat X_{1:0}, \mat Y_{1:0}) &= 0 \\
  D(\mat X_{1:0}, \mat Y_{1:k}) = D(\mat X_{1:k}, \mat Y_{1:0}) &= \infty.
\end{align*}
Padding with $\infty$ is required to ensure that the alignment starts with the first element in both sequences.

The way the recurrence is defined enables evaluation of $D(\mat X, \mat Y)$ in $O(nm)$ time using dynamic programming.
By saving backpointers during the calculation, it is also possible to retrieve the actual alignment.

\section{Machine learning}

Machine learning is the practice of automatically finding patterns in data, and use these patterns to make future predictions or perform decision making.
The learning process generally takes the form of setting up an appropriate mathematical or statistical model, and automatically changing the parameters of the model to fit the data.
Machine learning is commonly employed in a variety of fields such as speech recognition, computer vision and natural language processing (NLP).

Using text parsing in NLP as an example, machine learning provides several advantages over the classic approach of hand-engineered rules written by human experts, including:
\todo[inline]{make better arguments}
\begin{itemize}
 \item Grammatical rules are inferred based on actual data, rather than the expert's conception of how the language works.
 \item A computer can quickly go through an amount of data that would be far too vast for a human expert to analyse by hand.
 \item A machine learning model can incorporate statistical information learnt from large amounts of data, enabling it to return multiple possible interpretations along with confidence scores.
\end{itemize}
Similar advantages can be seen in other fields, such as speech recognition where it is simply not feasible to construct hand-written rules that can identify speech sounds from raw audio data.

This section will lightly touch upon machine learning concepts relevant to this thesis; for a proper introduction to the field, see \textcite{murphy2012machine}.

\subsection{Important concepts}
\subsubsection{Supervised and unsupervised learning}

A large number of techniques in machine learning can be broadly considered to be either supervised techniques, which take a set of data along with corresponding labels and try to learn the mapping from the data to the labels, or unsupervised techniques, which try to find ``interesting'' (as defined by the task at hand) patterns in unlabelled data.

As an example, consider the task of speech recognition.
The data set used to train our model consists of speech data along with a set of phonetic transcriptions, so that what is being said at each time instant is known.
Our task is to try to learn this mapping from speech to transcription, taking advantage of all available data.
This is a typical example of supervised learning, as we have a known ``ground truth'' that we are trying to replicate.

On the other hand, consider the case where our speech data is unlabelled, so that we do not know what is being said in a given utterance.
Without the ground truth we do not have a reference we can use to learn the mapping from speech signal to transcription.
Instead, our task is reduced to trying to find patterns in the data, by for example identifying repeating segments in the speech that could possibly correspond to speech sounds, or even whole words.
Note, however, that even if we are able to correctly identify words in the speech, we still do not know the corresponding orthographic transcription.
This is an example of unsupervised learning.

\subsubsection{Regression}

Regression is a supervised task where we are given input data $\mat x \in \mathbb{R}^n$---here real, though discrete input data is also common---and corresponding continuous output data $\mat y \in \mathbb{R}^m$, and our task is to find a function $f:\mathbb{R}^n \to \mathbb{R}^m$ that best preserves the relationship between input and output, and that can be used to predict output values for future input data.

A typical example might be predicting house prices, where the input data consists of information such as floor area, garden area, proximity to public transport, etc., and the output is a scalar indicating the price.

\subsubsection{Classification}

Classification is a special case of regression where the output is a discrete class.
Thus, the problem is finding a function $f:\mathbb{R}^n \to C$ where $C$ is the set of possible classes.
In some cases a data point may belong to more than one class at a time, in which case the mapping function can be defined as $f:\mathbb{R}^n \to \{0,1\}^c$, where $c$ is the number of possible classes, and $f(\mat x)_k$ is $1$ if $\mat x$ belongs to class $k$; this is known as multi-label classification.

A common classification task is image classification, where the objective is to identify the object or objects present in an image.
The input is the value of each pixel in the image, and the output is the class or set of classes corresponding to the object(s) in the image.

\subsubsection{Clustering}

Clustering is an unsupervised task, where the goal is to somehow group the input data into distinct classes, such that data points in one class are more similar to each other than to data points in other classes.
Both the concept of similarity and the interpretation of the different classes depends on the problem at hand.
One example of clustering is the grouping of speech frames generated from an audio signal into distinct phonetic classes, with no prior knowledge of what phonetic classes are available.

\subsubsection{Embedding}

In some cases we do not have access to the true class corresponding to a data point, but we \emph{do} have information on whether two given data points belong to the same class or not.
Using this information we wish to project the data into a new space where points belonging to the same class are close, while points belonging to different classes are distant.
This projection is referred to as an embedding, and the techinque has seen use in areas such as face recognition, where faces are projected onto a space where similarity can be measured more easily, and natural language processing, where words, sentences or even whole documents are converted into real-valued vectors that capture some semantic information.

\subsection{K-means clustering}

A simple approach to clustering is known as K-means clustering.
Here, $K$ cluster centers are initialised, often randomly, and then iteratively updated to better reflect the data.
At each iteration every data point is assigned to the cluster whose center is closest to it in terms of the Euclidean distance, and the center of each cluster is then set to the mean of the data points assigned to the cluster.
Once the cluster centers converge (i.e.\ stop updating), the training stops.

\subsection{Gaussian mixture models}

\todo[inline]{is $P(k)$ rigorous notation? also the pdf notation}
While K-means clustering is simple to implement, it also makes some hidden assumptions, such as each cluster being spherical, which make it a bad fit for many types of data.
A better assumption in many cases is that each data point was generated by one of $K$ multivariate Gaussian distributions, each Gaussian $k$ having mean $\boldsymbol \mu_k$ and covariance matrix $\boldsymbol \Sigma_k$.
The probability of the $k$th Gaussian generating a data point being $P(k \mid \boldsymbol \theta) = \pi_k$, where $\sum_{k=1}^K \pi_k = 1$.
The probability of seeing a data point $\mat x$ is then described by the probability density function
\[
f(\mat x \mid \boldsymbol \theta) = \sum_{k=1}^K P(k \mid \boldsymbol \theta)f(\mat x \mid k, \boldsymbol \theta) = \sum_{k=1}^K \pi_k \mathcal{N}(\mat x; \boldsymbol \mu_k,\, \boldsymbol \Sigma_k)
\]
where $\boldsymbol \theta$ is the parameters of the model:
\[
 \boldsymbol \theta = \{\pi_1, \pi_2, \dots, \pi_K, \boldsymbol \mu_1, \boldsymbol \mu_2, \dots, \boldsymbol \mu_K, \boldsymbol \Sigma_1, \boldsymbol \Sigma_2, \dots, \boldsymbol \Sigma_K\}.
\]

This is known as a (finite, as the number of components $K$ is set \emph{a priori}) Gaussian mixture model (GMM).
Clustering using a GMM is performed by initialising the parameters $\boldsymbol \theta$ to some (e.g.\ random) value, and then iteratively updating the parameters using the expectation maximisation (EM) algorithm to maximise the probability of the model having generated the data.
See \textcite{murphy2012machine} for a detailed description of EM for GMMs.

After training, the posterior distribution $P(k \mid \mat x, \boldsymbol \theta)$ can be calculated as
\[
P(k \mid \mat x, \boldsymbol \theta)
= \frac{P(k \mid \boldsymbol \theta)f(\mat x \mid k, \boldsymbol \theta)}{f(\mat x \mid \boldsymbol \theta)}
= \frac{P(k \mid \boldsymbol \theta)f(\mat x \mid k, \boldsymbol \theta)}{\sum_{l=1}^K P(l \mid \boldsymbol \theta)f(\mat x \mid l, \boldsymbol \theta)}
= \frac{\pi_k \mathcal{N}(\mat x; \boldsymbol \mu_k, \boldsymbol \Sigma_k)}{\sum_{l=1}^K \pi_l \mathcal{N}(\mat x; \boldsymbol \mu_l, \boldsymbol \Sigma_l)},
\]
giving the probability of $\mat x$ belonging to class $k$.

\section{Artificial neural networks}

Artificial neural networks (ANNs) are a family of machine learning models loosely inspired by biological neural networks.
Though different types of ANNs function quite differently from each other, a common theme is that they are composed of a network of units, or neurons, each performing a relatively simple task.
The power of the model comes from combining a large amount of units to form a single, complex model.

This section is mainly concerned with a specific type of neural network, namely the feedforward neural network.
The feedforward neural network is a regression function $f:X \to Y$ that takes an $n$-dimensional input $\mat x \in X$, $X \subseteq \mathbb{R}^n$ and returns an $m$-dimensional output $\mat y \in Y$, $Y \subseteq \mathbb{R}^m$.
Though the model is inherently a regressor, representing both input and output as real values, feedforward neural networks have been successfully applied to e.g.\ classification by interpreting the output $\mat y$ as a probability distribution, defining the probability of $\mat x$ belonging to class $k$ as $P(k \mid \mat x) = y_k$.

For a recent detailed text on ANNs in the context of deep learning, see \textcite{goodfellow2016deep}.

\subsection{Linear models}

Consider the problem of predicting $m$ scalar output variables $y_1, y_2, \dots, y_m$ using a weighted linear combination of $n$ input variables $x_1, x_2, \dots, x_n$: $y_j = \sum_{i=1}^n x_i w_{ij} + b_j$.
In matrix notation we write this as
\[
 \mat y = \mat x \mat W + \mat b
\]
where
\begin{align*}
  \mat x &= \left(\begin{matrix}x_1 & x_2 & \cdots & x_n\end{matrix}\right) \\
  \mat W &= \left(\begin{matrix}
    w_{11} & w_{12} & \cdots & w_{1m} \\
    w_{21} & w_{22} & \cdots & w_{2m} \\
    \vdots & \vdots & \ddots & \vdots \\
    w_{n1} & w_{n2} & \cdots & w_{nm}
  \end{matrix}\right) \\
  \mat b &= \left(\begin{matrix}b_1 & b_2 & \cdots & b_m\end{matrix}\right) \\
  \mat y &= \left(\begin{matrix}y_1 & y_2 & \cdots & y_m\end{matrix}\right).
\end{align*}

$\mat W$ is a weight matrix, defining a different weighted combination of input variables for each $y_j$.
$\mat b$ is a constant term known by a variety of names, including ``bias'', ``threshold'' and ``intercept'', depending on the context.
The interpretation of the bias is not wholly straightforward, but consider the case where each input variable $x_i$ has zero mean; in this case, $b_j$ represents the mean of $y_j$.

This model is known as \emph{linear regression}, or multivariate linear regression in the case of $m > 1$.
The model defines an $n$-dimensional hyperplane for each $y_j$, independently of the other output variables.
$y_j$ increases linearly along a steepest direction given by $\frac{\partial y_j}{\partial \mat x} = (w_{1j}, w_{2j}, \dots, w_{nj})$.

\begin{figure}
  \centering
  \input{figures/perceptron}
  \caption{\label{fig:perceptron}A simple linear model consisting of two layers.
  Each unit in the input layer corresponds to an input variable.
  Each output unit calculates a weighted sum of the input units.}
\end{figure}

A graphical illustration of the model is provided in \cref{fig:perceptron}.
We consider the model to be composed of two ``layers'': the input layer, $\mat x$, and the output layer, $\mat y$.
Each layer is additionally composed of ``units'', corresponding to the individual scalar variables $x_1,\dots,x_n,y_1,\dots,y_m$.
Every unit in the input layer is connected to every unit in the output layer, each connection representing a single weight.
The output units perform a summation of the weighted input variables before adding a bias value, yielding the final output.

Linear regression can be generalised by inserting a so-called ``activation function'' $g$ before outputting the final value:
\begin{align*}
 \mat y = g(\mat x \mat W + \mat b).
\end{align*}

If $g$ is chosen to be the logistic function $g(\mat z)_j = \sigma(z_j)$, this is known as \emph{logistic regression}.
The logistic function constrains the output to the range of $(0,1)$, allowing $y_j$ to be interpreted as modelling the Bernoulli distribution $P(j=1 \mid \mat x)$.

\subsection{Stacked linear models}

\begin{figure}[p]
  \centering
  \input{figures/separability}
  \caption{\label{fig:separability}A classic classification problem.
  Using our model we wish to correctly classify the blue and red data points.
  We take the output of our model to mean blue if it is positive, and red otherwise.
  \subref{fig:sep-linear} The model defines a plane in three-dimensional space.
  The model can only influence the direction, slant and position of the plane.
  \subref{fig:sep-tanh} A squashing function limits the range of the output to \numrange{-1}{1}.
  \subref{fig:sep-gaussian} Using a non-monotonic activation function (here the Gaussian function $\smash{\varphi(x) = \exp(-x^2)}$) it is possible to achieve more than one decision boundary, though all decision boundaries will be linear and parallel.
  However, by inserting an extra ``hidden'' layer between the input and output layers, the decision boundary can be made more complex.
  \subref{fig:sep-hidden1}--\subref{fig:sep-hidden2} The output of the units in the hidden layer.
  \subref{fig:sep-nonlinear} Using a linear combination of the hidden units, the resulting decision boundary perfectly separates the two classes.}
\end{figure}

While the generalized model is powerful in its own right, it has fatal drawbacks for certain types of data.
Consider the case of a one-dimensional output $y$ given by $y = g(\mat x \cdot \mat w + b)$, where $\mat w$ is a weight vector.
Let $\mat v$ be a vector orthogonal to $\mat w$, so that $\mat v \cdot \mat w = 0$.
We can now see that $y = g((\mat x + \mat v) \cdot \mat w + b) = g(\mat x \cdot \mat w + \mat v \cdot \mat w + b) = g(\mat x \cdot \mat w + b)$.
In other words, \textbf{linear models can only discriminate between data points along the axis defined by the weight vector}.
As a result of this, the model's decision boundaries will all be linear and parallel, meaning that it will only be able to classify data that is \emph{linearly seperable}.
The problem is illustrated in \crefrange{fig:sep-linear}{fig:sep-gaussian}.

However, we can extend our model to enable it to handle more complex data, by stacking several linear models on top of each other.
This is done by inserting a new layer, called a ``hidden'' layer, between the input and output layers.
Let $\mat y^0 = \mat x$ be the input layer, $\mat y^1$ the hidden layer, and $y^2 = y$ the output layer.
We let
\begin{align*}
 y^1_1 &= 2\varphi(y^0_1) - 1 \\
 y^1_2 &= 2\varphi(y^0_2) - 1 \\
 y^2 &= y^1_1 + y^1_2 - 0.5
\end{align*}
where $\varphi(x) = \exp(-x^2)$.
As can be seen in \crefrange{fig:sep-hidden1}{fig:sep-nonlinear}, this allows us to solve our classification problem.

\begin{figure}
  \centering
  \input{figures/feedforward}
  \caption{\label{fig:feedforward}A feedforward neural network with two hidden layers.
  As the data is moved through the hidden layers it is gradually transformed into a representation that hopefully enables the problem to be solved by the final linear model.}
\end{figure}

In general, our stacked model can be defined as
\begin{align*}
 \mat y^0 &= \mat x \\
 \mat y^l &= g^l(\mat y^{l-1} \mat W^l + \mat b^l) \quad l \in [1, N] \\
 \mat y &= \mat y^N
\end{align*}
where $N$ is the number of hidden or output layers, and $g^l$, $\mat W^l \in \mathbb{R}^{n^{l-1} \times n^l}$, $\mat b^l \in \mathbb{R}^{1 \times n^l}$ and $n^l$ are the activation function, weight matrix, bias vector and layer size (in number of units) corresponding to layer $l$, respectively.
This is known as a \emph{feedforward artificial neural network}; see \cref{fig:feedforward} for a graphical representation.
By setting
\begin{align*}
g^1(\mat z)_i &= 2\varphi(z_i) - 1 &
\mat W^1 &= \left(\begin{matrix}1 & 0 \\ 0 & 1\end{matrix}\right) &
\mat b^1 &= \mat 0 \\
%
g^2(z) &= z &
\mat W^2 &= \left(\begin{matrix}1 \\ 1\end{matrix}\right) &
b^2 &= -0.5 \\
\end{align*}
with $N = 2$, we obtain the previous example.

The number of hidden layers to use in a network deserves some consideration.
It has been shown that a feedforward network with a single hidden layer can approximate virtually any continuous function, as long as the number of hidden units is large enough \parencite{hornik1989multilayer}.
However, theoretical results suggest that the complexity of the model in terms of the types of functions it is able to express grows exponentially in the number of layers, meaning that a shallow network with only one hidden layer would need to consist of an exponential number of hidden units to match the complexity of a deep architecture \parencite{delalleau2011shallow,montufar2014number}.

\subsection{Activation functions}

It is possible to use different activation functions for different layers, or even different units within the same layer.
Typically, however, all hidden layers use the same activation function, with the activation function chosen for the output layer depending on the task at hand (e.g.\ classification, regression).

\subsubsection{Output layers}

For classification, an activation function that makes it possible to interpret the output as a one or more probability distributions is generally used.
A common choice is the softmax function, which normalises the output to sum to 1:
\[
\mathrm{softmax}(\mat z)_i = \frac{e^{z_i}}{\sum_j e^{z_j}}.
\]
For multi-label classification one can use the logistic function, defined as
\[
\sigma(\mat z)_i = \frac{1}{1 + e^{-z_i}}.
\]

When doing regression on the other hand, it is common to simply use a linear activation function:
\[
 g(\mat z) = \mat z.
\]

\subsubsection{Hidden layers}

It can be shown that a stacked model with linear activations in the hidden layers is equivalent to a shallow model with no hidden layers.
Thus, it is vital that the activation function used for the hidden layers be non-linear.
However, it can be advantageous to use a \emph{mostly linear} activation function, not least because both the function itself and its derivative becomes very fast to calculate.

Examples of mostly linear activation functions that have been shown to outperform other activation functions in many contexts include the rectified linear unit \parencite{glorot2011deep}:
\[
\mathrm{ReLU}(\mat z) = \max(\mat 0, \mat z)
\]
and the maxout unit \parencite{goodfellow2013maxout}:
\[
\mathrm{maxout}(\mat y^{l-1}) = \max(\mat y^{l-1} \mat W^l + \mat b^l, \mat y^{l-1} \mat V^l + \mat c^l)
\]
where $\max$ is performed element-wise, i.e.\ $\max(\mat u, \mat v)_i = \max(u_i, v_i)$.
Note that the input to maxout is the output of the previous layer rather than a weighted sum; it is a generalisation of the ReLU, keeping multiple (not necessarily limited to only two) sets of weight and bias values.

\todo[inline]{add source (german??) or remove as digression}
\todo[inline]{weights play a large role too...}
Historically it has also been common to use sigmoidal (S-shaped) functions such as the logistic function or $\tanh$ for hidden layers as well, as a way to approximate the spiking behaviour of biological neurons.
However, a major issue with a sigmoidal activation function is that, in addition to being comparatively slow to compute, the derivative of the function quickly falls towards 0 as the function saturates for large positive and negative input values, which tends to occurr as training progresses.
This causes training of the network to slow down, especially in networks with many hidden layers where the derivatives of multiple activation functions are multiplied, in a phonemonen known as the ``vanishing gradient'' problem.

\subsection{Training neural networks}

In order to produce any useful results, the parameters (weights and biases) $\boldsymbol \theta$ of the network need to be tuned.
This tuning is referred to as \emph{training} the network, and is performed by minimising some cost function (also known as a loss function or objective function) $L(\mat x, \mat y; \boldsymbol \theta)$ where $\mat x$ is the input to the network, and $\mat y$ is the \emph{expected} output of the network.
If $\hat{\mat y} = f(\mat x; \boldsymbol \theta)$ is the actual output of the network, $L(\mat x, \mat y; \boldsymbol \theta)$ defines some measure of how dissimilar $\hat{\mat y}$ and $\mat y$ are.

For classification, the most common cost function is the cross-entropy between $\mat y$ and $\mat x$, defined as
\[
 L_{\mathrm{CE}}(\mat x, \mat y; \boldsymbol \theta) = -\sum_{j=1}^m y_j \log \hat y_j.
\]
For single-class classification such that one output is 1 and all others 0, this simplifies to
\[
 L_{\mathrm{CE}}(\mat x, \mat y; \boldsymbol \theta) = -\log \hat y_j
\]
for the $j$ such that $y_j = 1$.
Minimising the cross-entropy between $\mat y$ and $\mat x$ with respect to $\mat x$ is equivalent to minimising the Kullback--Leibler divergence between the same, making it appropriate as a cost function when interpreting the network output as a probability distribution.

Regression often makes use of the mean squared error (sometimes multiplied by a factor of $\frac{1}{2}$) instead:
\[
L_{\mathrm{MSE}}(\mat x, \mat y; \boldsymbol \theta) = \frac{1}{m} \lVert\hat {\mat y} - \mat y\rVert_2^2.
\]
Autoencoders, which attempt to find a low-dimensional representation of the input data from which the data can be reconstructed, use $L_{\mathrm{MSE}}(\mat x, \mat x; \boldsymbol \theta)$, called the reconstruction error, as the cost function.

Once the cost function is defined, the network can be trained by iteratively modifying the network parameters through gradient descent.
Usually a special form of gradient descent called mini-batch gradient descent is used, where the network is presented with a small subset of the examples at each iteration, and the gradient is averaged over the presented examples.
Let $B$ be the number of examples per iteration (the batch size) and $(\mat x_1, \mat y_1), (\mat x_2, \mat y_2), \dots, (\mat x_B, \mat y_B)$ be $B$ training examples to be presented to the network this batch.
We can the state the training procedure as
\[
\boldsymbol \theta^{t+1} \gets \boldsymbol \theta^t - \eta\frac{\partial}{\partial \boldsymbol \theta}\left(\frac{1}{B} \sum_{i=1}^B L(\mat x_i, \mat y_i; \boldsymbol \theta^t)\right)
\]
where $\eta$ is the ``learning rate''.
Tuning the learning rate is important, as a high learning rate means faster learning, but setting it too high may prevent the training from converging.

By exploiting the layered structure of feedforward neural networks, it is possible to calculate the gradient efficiently in a recursive manner.
This algorithm is commonly referred to as the backpropagation algorithm \parencite{rumelhart1986learning}.
