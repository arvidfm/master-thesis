% Copyright (C) 2016  Arvid Fahlström Myrman
%
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License as published by
% the Free Software Foundation; either version 2 of the License, or
% (at your option) any later version.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
%
% You should have received a copy of the GNU General Public License along
% with this program; if not, write to the Free Software Foundation, Inc.,
% 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

\chapter{Theory}
\label{ch:theory}

This \lcnamecref{ch:methods} provides an overview of topics that serve as a base for the rest of the thesis.
Topics covered include speech recognition, basic concepts in machine learning, and artificial neural networks.

\section{Processing speech signals}

An important step in most speech recognition applications is feature extraction: From a raw audio signal we wish to extract information (features) that can be used to efficiently process or model the speech to achieve some desired result.
As the exact types of features used vary depending on the model and the goal of the application, this section will focus on some particularly common features used in speech recognition.
Additionally, some standard methods of processing speech that take into consideration the sequential and dynamic nature of speech will be discussed.

This section serves as a short introduction to signal processing for speech recognition, and only covers the steps necessary to extract the features used to train the Gaussian mixture model used for this work, as well as dynamic time warping (DTW), which is used to find frame-level by aligning same-word fragments.
For a more in-depth description, see a book on signal processing such as \textcite{quatieri2002discrete}, or see \textcite{huang2001spoken} for an introduction to speech recognition in particular.

\subsection{Audio signals}

In the real world speech takes the form of a pressure wave generated when air is pushed through the vocal tract.
The pressure wave as perceived from a single point in space can be described as a continuous signal $x(t), t\in \mathbb{R}$ that varies smoothly in time, with $x(t)$ at each time $t$ describing the amplitude of the wave relative to the ambient pressure.
Our perception of the signal depends not on the absolute value of $x(t)$ at any given time instant, but rather on how it varies in time.
As an example, take a simple sine wave $x(t) = \sin(2\pi f t)$; though the signal oscillates continuously in time, a human would perceive the signal as a single constant sound of frequency $f$.

However, while the physical signal is continuous, digital computers are unable to handle signals with an infinitely high time resolution, and as a consequence the signal must be somehow discretised before it can be processed further by speech applications.
This is done by taking samples of the signal at fixed time steps given by a sampling frequency $f_s$ (e.g.\ \SI{16000}{\Hz}), specifying the number of samples taken per second.
The resulting sampled signal $x[n], n \in \mathbb{Z}$ is a discrete-time approximation of the original signal $x(t)$.

\subsection{Short-time Fourier transform}

To mirror how humans perceive audio signals it is useful to convert the signal to some form that better captures its fluctuations.
One way is to analyse the \emph{frequency content} of the signal using the Fourier transform.
The Fourier transform approximates the signal using a sum of sine and cosine waves of different frequencies, giving the amplitude of each such wave, which in turn can be interpreted as the energy content of the signal at the corresponding frequency.
In particular we define the discrete Fourier transform (DFT), which gives the frequency content of a finite discrete-time signal of length $N$ samples, as
\begin{equation}
X(k) = \sum_{n=0}^{N-1} x[n]e^{-j2\pi kn/N},
\end{equation}
where $j$ is the imaginary unit and $k \in [0, N-1]$ corresponds to the frequency $f = \frac{k}{N}f_s$, $f_s$ being the sampling frequency of the signal.
The energy density, or the energy distribution over frequency, is given by $S(k) = \left|X(k)\right|^2$.
The DFT can be calculated in $O(N \log N)$ asymptotic time using the fast Fourier transform (FFT) algorithm \parencite{cooley1965algorithm}.

\begin{figure}
  \centering
  \input{figures/stft}
  \caption{\label{fig:sftf}Example of running a Fourier transform on a \SI{25}{\ms} section of a recording of the word ``bed''.
  The dashed line shows the ``envelope'', or overall shape, of the energy spectrum.
  The peaks and valleys of the spectrum are characteristic of sonorant sounds such as vowels.}
\end{figure}

The DFT makes certain assumptions regarding the nature of the signal.
In particular, it assumes that the signal is periodic, which is emphatically not true in general for speech signals.
However, a speech signal can be thought to be \emph{approximately periodic} over a very short time period.
This gives rise to the so-called short-time Fourier transform (STFT), where the DFT is calculated repeatedly on short sections of the signal using a sliding window; see \cref{fig:sftf} for an example.
A typical window length is \SI{25}{\ms}, and it is generally shifted forward about \SI{10}{\ms} between each DFT calculation.
The result of the STFT is the 2D Fourier transform $X(m,k)$ over time step $m$ and frequency $k$, with corresponding energy density $S(m,k) = \left|X(m, k)\right|^2$.
An illustration of the STFT can be seen in \cref{fig:spectrogram}.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[enlargelimits=false,axis on top,width=8cm,height=4cm,ylabel={Frequency (\si\Hz)},xlabel={Time (\si\s)}]
      \addplot graphics[xmin=0,xmax=2.8,ymin=0,ymax=4700] {data/spectrogram.pdf};
    \end{axis}
  \end{tikzpicture}

  \caption{\label{fig:spectrogram}The result of running the STFT on a \SI{3}{\s} long signal.}
\end{figure}

\subsection{Mel-scale filter banks}

There are several problems with working directly with the output of the STFT. One is that the output is very high-dimensional, as $N$ is often chosen to be in the \numrange{512}{2048} range for the DFT.
Unless very large amounts of data is available, this causes data sparsity issues, which can cause many kinds of models to underperform.
Additionally, speech sounds are better described by the general shape of the spectrum rather than the exact energy at every frequency step.

\begin{figure}
  \centering
  \input{figures/mel-scale}
  \caption{\label{fig:melscale}The mel scale as it corresponds to the standard frequency scale.
  Higher frequencies are closer together on the mel scale than lower frequencies.}
\end{figure}

\begin{figure}
  \centering
  \input{figures/mel-filterbank}
  \caption{\label{fig:filterbank}20 triangular filters spaced linearly along the mel scale from \SIrange{0}{8000}{\Hz}.
  The peaks of the filters are scaled as to ensure constant area.}
\end{figure}

Finally, all frequencies are not created equal, as the human ear does not discriminate between higher frequencies to the same extent as between lower frequencies.
To model this phenomenon, scales in which a change in pitch corresponds roughly linearly to the subjective change in pitch perceived by a human have been empirically developed.
One such scale, ubiquitous in speech technology, is the mel scale, which was developed through experiments where participants were told to produce a tone with half the perceived pitch of a reference tone \parencite{stevens1937scale}.
A frequency $f$ can be converted to the mel scale through the following relation:
\begin{equation}
  \mathrm{mel}(f) = 1127\log\left(1 + \frac{f}{700}\right)
\end{equation}
where $\log$ is the natural logarithm.
As can be seen in \cref{fig:melscale}, as $f$ grows larger, the difference $\mathrm{mel}(f+\varepsilon) - \mathrm{mel}(f)$ becomes smaller.

Addressing both the issues of dimensionality and perception, we construct the mel-scale filter bank.
The filter bank is a set of $L$ triangular filters, with the middle points of the filters spaced linearly on the mel scale.
In other words, if $f_1, f_2, \dots, f_L$ are the middle points of the filters specified in \si{\Hz}, $\mathrm{mel}(f_k) - \mathrm{mel}(f_{k-1}) = \mathrm{mel}(f_{k+1}) - \mathrm{mel}(f_k)$, $k \in [2, L-1]$.
The start and end points of the filters are the middle points of the previous and following filters, respectively, with the exception of the first filter whose start point is specified by a lower bound $f_{low}$, and the last filter whose end point is given by a higher bound $f_{high}$.
The height of the peak of each filter can vary; common approaches are to ensure that the filters have either constant height or constant area.
An illustration of a filter bank is given in \cref{fig:filterbank}.
Each filter is applied to the energy spectrum of the audio signal, giving the filter bank output $E(m,l) = \sum_{k=0}^{N-1}V_l(k)S(m,k)$, where $V_l(k)$ is the value of the $(l+1)$th mel-scale filter at frequency $k$.
The resulting $L$ values at each time step $m$ form a rough approximation of the energy spectrum, with the resolution being higher for low frequencies than for high frequencies.
$L$ is commonly chosen to be in the \numrange{20}{40} range, significantly lowering the dimensionality of the data.

\subsection{Mel frequency cepstral coefficients}

In certain applications, it is beneficial if the features generated are relatively decorrelated.
For instance, this is the case when modelling the features using multivariate Gaussian distributions, where if the features are decorrelated the covariance matrix can be approximated using a diagonal matrix, significantly reducing the number of parameters that need to be learnt.

Unfortunately, the mel-scale filter bank outputs are \emph{not} decorrelated, as neighbouring frequencies tend to take on similar values in the energy spectrum.
However, they can be made roughly decorrelated by taking the discrete cosine transform (DCT) of the logarithm of the filter bank outputs, given as
\begin{equation}
C(m,c) = \sum_{l=0}^{L-1} \log(E(m,l)) \cos\left[\frac{\pi}{L}\left(l+\frac{1}{2}\right)c\right]
\end{equation}
for $c \in [0, L - 1]$.
The DCT approximates the input using cosine waves in a similar manner to how the Fourier transform works, with the $c$th coefficient at each time step indicating how similar the input is to a cosine wave with $c/2$ periods as $l$ goes from $0$ to $L-1$; for $c=0$ this wave is a constant function.

The resulting values are known as the mel-frequency cepstral coefficients (MFCCs).
Usually only the first 13 or so coefficients are used at each time step, i.e.\ the coefficients corresponding to $c \in [0, 12]$, once again reducing the dimensionality of the data.

\subsection{Modelling evolution over time}

Speech is inherently sequential.
This means that our perception of speech is not dependent on particular absolute values of the speech signal at particular points in time, but rather on how the signal evolves over time, and the exact realisation of individual speech sounds is formed through complex interplay with neighbouring sounds.
In addition, many speech sounds change over time by nature; examples include the diphthong /aɪ/ in the word \emph{my} /maɪ/, or the affricate /tʃ/ in \emph{teach} /tiːtʃ/, both acting as single units of speech, despite the onset and offset of the sounds being significantly different acoustically.

Thus, it is a difficult task to recognise a speech sound based only on a single frame of audio.
Instead, the model needs some way of incorporating information about the context of the frame.
This can be done both at the model level and the feature level.
Model-based approaches include hidden Markov models (HMMs), which encode information about how the speech can change in time in the form of probabilities, and recurrent neural networks (RNNs), which can take sequences as input and automatically find temporal patterns.

Feature-based approaches, instead, include temporal information directly in the features.
One simple way is to extend each speech frame to include not only the current frame, but also the neighbouring frames before feeding it to the model.
For instance, if our features consisted of the outputs of a mel-scaled filter bank of size 40 at different time steps $m$ and we wanted to include a context of 2 frames in both directions it time, the frame at each $m$ would be extended to include the frames at $m-2$, $m-1$, $m$, $m+1$ and $m+2$, resulting in a feature vector of size $40\cdot5 = 200$ at each time step.

Another feature-based approach is to include approximations of the temporal derivatives in the feature vector, most commonly the first-order (velocity) and second-order (acceleration) derivatives.
Let $\mat y = (y_t)_{t=1}^N$ be a sequence of feature vectors, and $n$ the number of points on both sides of $y_t$ to use to approximate the temporal derivative at $t$.
The approximate derivative, or \emph{delta} value, at $t$ is then given by
\begin{equation}
\Delta y_t \approx \frac{\sum_{k=1}^n k(y_{t+k} - y_{t-k})}{2 \sum_{k=1}^n k^2}
\end{equation}
which is the formula used by toolkits such as HTK \parencite{young2005htk}.
The second-order derivative, or the \emph{delta-delta} values, can be obtained by repeating the process using the delta values.
A detailed derivation of this formula can be found in \cref{app:delta}.

\subsection{Dynamic time warping}

Rather than training a model to perform speech recognition, it is sometimes of interest to directly measure the similarity of two utterances.
However, the dynamic nature of speech makes this difficult: A single speaker if asked to repeat a word two times will not pronounce the word exactly the same both times, and the length of the utterances will also differ slightly.
Dynamic time warping (DTW) attempts to deal with this by finding the best alignment of the frames of the two utterances and measure the similarity based on this alignment.

Let $d(\mat x, \mat y)$ be some local measure of the distance between the feature vectors $\mat x$ and $\mat y$, e.g.\ the Euclidean distance.
Let $\mat X = \{\mat x_1, \mat x_2, \dots, \mat x_n\}$ and $\mat Y = \{\mat y_1, \mat y_2, \dots, \mat y_m\}$ be sequences of feature vectors, and let $\mat X_{k:l}$ denote the subsequence of $\mat X$ starting at $k$ and ending at $l$. 
We wish to find a global distance measure $D(\mat X, \mat Y)$ as a sum of the local distances between the feature vectors, based on some alignment that minimises this distance.
All elements in both sequences must be used in the final alignment, but elements may be repeated at will to serve as padding.

This can be expressed through the recurrence
\begin{align}
D(\mat X_{1:k}, \mat Y_{1:l}) = d(\mat x_k, \mat y_l) + \min(&D(\mat X_{1:k-1}, \mat Y_{1:l-1}), \nonumber \\
& D(\mat X_{1:k-1}, \mat Y_{1:l}), \nonumber \\
& D(\mat X_{1:k}, \mat Y_{1:l-1}))
\end{align}
with the base cases
\begin{align}
  D(\mat X_{1:0}, \mat Y_{1:0}) &= 0 \\
  D(\mat X_{1:0}, \mat Y_{1:k}) = D(\mat X_{1:k}, \mat Y_{1:0}) &= \infty.
\end{align}
Padding with $\infty$ is required to ensure that the alignment starts at the first elements of both sequences.

The way the recurrence is defined enables evaluation of $D(\mat X, \mat Y)$ in $O(nm)$ time using dynamic programming.
By saving backpointers during the calculation, it is also possible to retrieve the actual alignment.

\section{Machine learning}
\label{sec:machine-learning}

Machine learning can roughly be described as the practice of automatically finding patterns in data, and using these patterns to make future predictions or perform decision making.
The learning process generally takes the form of setting up an appropriate mathematical or statistical model, and automatically changing the parameters of the model to fit the data.
Machine learning is commonly employed in a variety of fields such as speech recognition, computer vision and natural language processing (NLP).

As an illustration of the benefits of machine learning, consider the field of computer vision, and in particular the task of recognising objects in photographs.
While an object such as a tree is easy for a human to recognise, conditions such as lighting, camera equipment, and general noise can greatly effect the raw values of the pixels in a photo.
This makes it difficult if not impossible to create hand-written rules to recognise objects, in terms of e.g. the colours of the pixels.
This is in contrast to classical applications of artificial intelligence such as chess, where complete, noise-free information about the board state is available at all times, which facilitates hand-written heuristics or strategies such as exhaustive search.

A machine learning model, on the other hand, can learn to recognise patterns, such as objects in images, without explicit instruction on how this recognition should be performed.
By presenting the model with example data, e.g.\ in the form of input and expected output, the model automatically finds statistical patterns and relations in the data.
This makes it possible to use vast amounts of data to quickly find patterns in complex data such as images or speech.

This \lcnamecref{sec:machine-learning} will lightly touch upon machine learning concepts relevant to this thesis, starting with general concepts needed to better understand the background of this work, along with a brief description of Gaussian mixture models, used in this work to generate features to serve as input to the proposed model.
For a proper introduction to the field, see \textcite{murphy2012machine}.

\subsection{Important concepts}
\subsubsection{Supervised and unsupervised learning}

A large number of techniques in machine learning can be broadly considered to be either supervised techniques, which take a set of data along with corresponding labels and try to learn the mapping from the data to the labels, or unsupervised techniques, which try to find ``interesting'' (as defined by the task at hand) patterns in unlabelled data.

As an example, consider the task of speech recognition.
The data set used to train our model consists of speech data along with a set of transcriptions, so that what is being said in each utterance is known.
Our task is to try to learn this mapping from speech to transcription, taking advantage of all available data.
This is a typical example of supervised learning, as we have a known ``ground truth'' that we are trying to replicate.

On the other hand, consider the case where our speech data is unlabelled, so that we do not know what is being said in a given utterance.
Without the ground truth we do not have a reference we can use to learn the mapping from speech signal to transcription.
Instead, our task is reduced to trying to find patterns in the data, by for example identifying repeating segments in the speech that could possibly correspond to speech sounds, or even whole words.
Note, however, that even if we are able to correctly identify words in the speech, we still do not know the corresponding orthographic transcription.
This is an example of unsupervised learning.

The difference between supervised and unsupervised learning is not always clear-cut, and some tasks can be considered more supervised than others.
For instance, in speech recognition we may only have access to an orthographical transcription of a given utterance, with no information available regarding what sound the individual speech frames correspond to.
On the other hand, in computer vision we often do know the true output for each individual input image.
The amount of information available in the data strongly influences what kinds of models and algorithms are available at our disposal.

\subsubsection{Regression}

Regression is a supervised task where we are given input data $\mat x \in \mathbb{R}^n$---here real, though discrete input data is also common---and corresponding continuous output data $\mat y \in \mathbb{R}^m$, and our task is to find a function $f:\mathbb{R}^n \to \mathbb{R}^m$ that best preserves the relationship between input and output, and that can be used to predict output values for future input data.

A typical example might be predicting house prices, where the input data consists of information such as floor area, garden area, proximity to public transport, etc., and the output is a scalar indicating the price.

\subsubsection{Classification}

Classification is a special case of regression where the output is a discrete class.
Thus, the problem is finding a function $f:\mathbb{R}^n \to C$ where $C$ is the set of possible classes.
In some cases a data point may belong to more than one class at a time, in which case the mapping function can be defined as $f:\mathbb{R}^n \to \{0,1\}^c$, where $c$ is the number of possible classes, and $f(\mat x)_k$ is $1$ if $\mat x$ belongs to class $k$; this is known as multi-label classification.

A common classification task is image classification, where the objective is to identify the object or objects present in an image.
The input is the value of each pixel in the image, and the output is the class or set of classes corresponding to the object(s) in the image.

\subsubsection{Clustering}

Clustering is an unsupervised task, where the goal is to somehow group the input data into distinct classes, such that data points in one class are more similar to each other than to data points in other classes.
Both the concept of similarity and the interpretation of the different classes depends on the problem at hand.
One example of clustering is the grouping of speech frames generated from an audio signal into distinct phonetic classes, with no prior knowledge of what phonetic classes are available.

\subsubsection{Embedding}

In some cases we do not have access to the true class corresponding to a data point, but we \emph{do} have information such as whether two given data points belong to the same class or not, or what context the data points appear in.
Using this information we wish to project the data onto a new space where similar data points are close, while dissimilar points are distant.
This projection is referred to as an embedding, and the techinque has seen use in areas such as face recognition, where faces are projected onto a space where similarity can be measured more easily, and natural language processing, where words, sentences or even whole documents are converted into real-valued vectors that capture some semantic information.
In this work, the output of the proposed model can be seen as an embedded representation of the input speech frames.

\subsection{K-means clustering}

A simple approach to clustering is known as K-means clustering.
Here, $K$ cluster centers are initialised, often randomly, and then iteratively updated to better reflect the data.
At each iteration every data point is assigned to the cluster whose center is closest to it in terms of the Euclidean distance, and the center of each cluster is then set to the mean of the data points assigned to the cluster.
Once the cluster centers converge (i.e.\ stop updating), the training stops.

\subsection{Gaussian mixture models}

While K-means clustering is simple to implement, it also makes some hidden assumptions, such as each cluster being spherical with the same variance, which do not hold for many types of real-world data, including speech data.
A better assumption in many cases is that each data point was generated by one of $K$ multivariate Gaussian distributions, each Gaussian $k$ having mean $\boldsymbol \mu_k$ and covariance matrix $\boldsymbol \Sigma_k$.
We denote the probability of the $k$th Gaussian generating a data point as $p(k \mid \boldsymbol \theta) = \pi_k$, where $\sum_{k=1}^K \pi_k = 1$.
The probability of seeing a data point $\mat x$ is then described by
\begin{equation}
p(\mat x \mid \boldsymbol \theta) = \sum_{k=1}^K p(k \mid \boldsymbol \theta)p(\mat x \mid k, \boldsymbol \theta) = \sum_{k=1}^K \pi_k \mathcal{N}(\mat x; \boldsymbol \mu_k,\, \boldsymbol \Sigma_k)
\end{equation}
where $\boldsymbol \theta$ is the parameters of the model:
\begin{equation}
 \boldsymbol \theta = \{\pi_1, \pi_2, \dots, \pi_K, \boldsymbol \mu_1, \boldsymbol \mu_2, \dots, \boldsymbol \mu_K, \boldsymbol \Sigma_1, \boldsymbol \Sigma_2, \dots, \boldsymbol \Sigma_K\}.
\end{equation}

This is known as a (finite, as the number of components $K$ is set \emph{a priori}) Gaussian mixture model (GMM).
Clustering using a GMM is performed by initialising the parameters $\boldsymbol \theta$ to some (e.g.\ random) value, and then iteratively updating the parameters using the expectation maximisation (EM) algorithm to maximise the probability of the model having generated the data.
See \textcite{murphy2012machine} for a detailed description of EM for GMMs.

After training, the posterior distribution $p(k \mid \mat x, \boldsymbol \theta)$ can be calculated as
\begin{align}
p(k \mid \mat x, \boldsymbol \theta)
&= \frac{p(k \mid \boldsymbol \theta)p(\mat x \mid k, \boldsymbol \theta)}{p(\mat x \mid \boldsymbol \theta)}
= \frac{p(k \mid \boldsymbol \theta)p(\mat x \mid k, \boldsymbol \theta)}{\sum_{l=1}^K p(l \mid \boldsymbol \theta)p(\mat x \mid l, \boldsymbol \theta)} \nonumber \\
&= \frac{\pi_k \mathcal{N}(\mat x; \boldsymbol \mu_k, \boldsymbol \Sigma_k)}{\sum_{l=1}^K \pi_l \mathcal{N}(\mat x; \boldsymbol \mu_l, \boldsymbol \Sigma_l)},
\end{align}
giving the probability of $\mat x$ belonging to class $k$.

\section{Artificial neural networks}
\label{sec:ann}

Artificial neural networks (ANNs) are a family of machine learning models loosely inspired by biological neural networks.
Though different types of ANNs function quite differently from each other, a common theme is that they are composed of a network of units, or neurons, each performing a relatively simple task such as a weighted sum.
The power of the model comes from combining a large amount of units to form a single, complex model.

This \lcnamecref{sec:ann} is mainly concerned with a specific type of neural network, namely the feedforward neural network.
The feedforward neural network is a regression function $f:X \to Y$ that takes an $n$-dimensional input $\mat x \in X$, $X \subseteq \mathbb{R}^n$ and returns an $m$-dimensional output $\mat y \in Y$, $Y \subseteq \mathbb{R}^m$.
Though the model is inherently a regressor, representing both input and output as real values, feedforward neural networks have been successfully applied to e.g.\ classification by interpreting the output $\mat y$ as a probability distribution, defining the probability of $\mat x$ belonging to class $k$ as $p(k \mid \mat x) = y_k$.

We describe both linear models, which the model proposed in this work builds upon, and deep models, which one of the related models described is based on, with the training procedure being similar for both types of models.
For a recent detailed text on ANNs in the context of deep learning, see \textcite{goodfellow2016deep}.

\subsection{Linear models}

Consider the problem of predicting $m$ scalar output variables $y_1, y_2, \dots, y_m$ using a weighted linear combination of $n$ input variables $x_1, x_2, \dots, x_n$: $y_j = \sum_{i=1}^n x_i w_{ij} + b_j$.
In matrix notation we write this as
\begin{equation}
 \mat y = \mat x \mat W + \mat b
\end{equation}
where
\begin{align}
  \mat x &= \left(\begin{matrix}x_1 & x_2 & \cdots & x_n\end{matrix}\right) \\
  \mat W &= \left(\begin{matrix}
    w_{11} & w_{12} & \cdots & w_{1m} \\
    w_{21} & w_{22} & \cdots & w_{2m} \\
    \vdots & \vdots & \ddots & \vdots \\
    w_{n1} & w_{n2} & \cdots & w_{nm}
  \end{matrix}\right) \\
  \mat b &= \left(\begin{matrix}b_1 & b_2 & \cdots & b_m\end{matrix}\right) \\
  \mat y &= \left(\begin{matrix}y_1 & y_2 & \cdots & y_m\end{matrix}\right).
\end{align}

$\mat W$ is a weight matrix, defining a different weighted combination of input variables for each $y_j$.
$\mat b$ is a constant term variably known as ``bias'', ``threshold'' or ``intercept'', depending on the context.
The interpretation of the bias is not wholly straightforward, but consider the case where each input variable $x_i$ has zero mean; in this case, $b_j$ represents the mean of $y_j$.

This model is known as \emph{linear regression}, or multivariate linear regression in the case of $m > 1$.
The model defines an $n$-dimensional hyperplane for each $y_j$, independently of the other output variables.
$y_j$ increases linearly with respect to $\mat x$ along a steepest direction given by $\frac{\partial y_j}{\partial \mat x} = (w_{1j}, w_{2j}, \dots, w_{nj})$.

\begin{figure}
  \centering
  \input{figures/perceptron}
  \caption{\label{fig:perceptron}A simple linear model consisting of two layers.
  Each unit in the input layer corresponds to an input variable.
  Each output unit calculates a weighted sum of the input units.}
\end{figure}

A graphical illustration of the model is provided in \cref{fig:perceptron}.
We consider the model to be composed of two ``layers'': the input layer, $\mat x$, and the output layer, $\mat y$.
Each layer is additionally composed of ``units'', corresponding to the individual scalar variables $x_1,\dots,x_n,y_1,\dots,y_m$.
Every unit in the input layer is connected to every unit in the output layer, each connection representing a single weight.
The output units perform a summation of the weighted input variables before adding a bias value, yielding the final output.

Linear regression can be generalised by inserting a so-called ``activation function'' $g$ before outputting the final value:
\begin{equation}
 \mat y = g(\mat x \mat W + \mat b).
\end{equation}

If $g$ is chosen to be the logistic function $g(\mat z)_j = \sigma(z_j)$, defined in \cref{eq:logistic}, this is known as \emph{logistic regression}.
The logistic function constrains the output to the range of $(0,1)$, allowing $y_j$ to be interpreted as modelling the Bernoulli distribution $p(j=1 \mid \mat x)$.

\subsection{Stacked linear models}

\begin{figure}[p]
  \centering
  \input{figures/separability}
  \caption{\label{fig:separability}A classic classification problem.
  Using our model we wish to correctly classify the blue and red data points.
  We take the output of our model to mean blue if it is positive, and red otherwise.
  \subref{fig:sep-linear} The model defines a plane in three-dimensional space.
  The model can only influence the direction, slant and position of the plane.
  \subref{fig:sep-tanh} A squashing function limits the range of the output to \numrange{-1}{1}.
  \subref{fig:sep-gaussian} Using a non-monotonic activation function (here the Gaussian function $\smash{\varphi(x) = \exp(-x^2)}$) it is possible to achieve more than one decision boundary, though all decision boundaries will be linear and parallel.
  However, by inserting an extra ``hidden'' layer between the input and output layers, the decision boundary can be made more complex.
  \subref{fig:sep-hidden1}--\subref{fig:sep-hidden2} The output of the units in the hidden layer.
  \subref{fig:sep-nonlinear} Using a linear combination of the hidden units, the resulting decision boundary perfectly separates the two classes.}
\end{figure}

While the generalised model is powerful in its own right, it has fatal drawbacks for certain types of data.
Consider the case of a one-dimensional output $y$ given by $y = g(\mat x \cdot \mat w + b)$, where $\mat w$ is a weight vector.
Let $\mat v$ be a vector orthogonal to $\mat w$, so that $\mat v \cdot \mat w = 0$.
We can now see that $y = g((\mat x + \mat v) \cdot \mat w + b) = g(\mat x \cdot \mat w + \mat v \cdot \mat w + b) = g(\mat x \cdot \mat w + b)$.
In other words, \textbf{linear models can only discriminate between data points along the axis defined by the weight vector}.
As a result, the model's decision boundaries are all linear and parallel, meaning that it is only able to classify data that is \emph{linearly seperable}.
The problem is illustrated in \crefrange{fig:sep-linear}{fig:sep-gaussian}.

However, we can extend our model to enable it to handle more complex data, by stacking several linear models on top of each other.
This is done by inserting a new layer, called a ``hidden'' layer, between the input and output layers.
Let $\mat y^0 = \mat x$ be the input layer, $\mat y^1$ the hidden layer, and $y^2 = y$ the output layer.
We let
\begin{align}
 y^1_1 &= 2\varphi(y^0_1) - 1 \\
 y^1_2 &= 2\varphi(y^0_2) - 1 \\
 y^2 &= y^1_1 + y^1_2 - 0.5
\end{align}
where $\varphi(x) = \exp(-x^2)$.
As can be seen in \crefrange{fig:sep-hidden1}{fig:sep-nonlinear}, this allows us to solve our classification problem.

\begin{figure}
  \centering
  \input{figures/feedforward}
  \caption{\label{fig:feedforward}A feedforward neural network with two hidden layers.
  As the data is moved through the hidden layers it is gradually transformed into a representation that hopefully enables the problem to be solved by the final linear model.}
\end{figure}

In general, our stacked model can be defined as
\begin{align}
 \mat y^0 &= \mat x \\
 \mat z^l &= \mat y^{l-1} \mat W^l + \mat b^l \\
 \mat y^l &= g^l(\mat z^l) \\
 \mat y &= \mat y^N
\end{align}
where $l \in [1, N]$; $N$ is the number of hidden or output layers, and $g^l$, $\mat W^l \in \mathbb{R}^{n^{l-1} \times n^l}$, $\mat b^l \in \mathbb{R}^{1 \times n^l}$ and $n^l$ are the activation function, weight matrix, bias vector and layer size (in number of units) corresponding to layer $l$, respectively.
This is known as a \emph{feedforward artificial neural network}; see \cref{fig:feedforward} for a graphical representation.
By setting
\begin{align}
g^1(\mat z)_i &= 2\varphi(z_i) - 1 &
\mat W^1 &= \left(\begin{matrix}1 & 0 \\ 0 & 1\end{matrix}\right) &
\mat b^1 &= \mat 0 \\
%
g^2(z) &= z &
\mat W^2 &= \left(\begin{matrix}1 \\ 1\end{matrix}\right) &
b^2 &= -0.5
\end{align}
with $N = 2$, we obtain the previous example.

The number of hidden layers to use in a network deserves some consideration.
It has been shown that a feedforward network with a single hidden layer can approximate a large class of continuous functions, as long as the number of hidden units is large enough \parencite{hornik1989multilayer}.
However, theoretical results suggest that the complexity of the model in terms of the types of functions it is able to express grows exponentially in the number of layers, meaning that a shallow network with only one hidden layer would need to consist of an exponential number of hidden units to match the complexity of a deep architecture \parencite{montufar2014number}.

\subsection{Activation functions}

It is possible to use different activation functions for different layers, or even different units within the same layer.
Typically, however, all hidden layers use the same activation function, with the activation function chosen for the output layer depending on the task at hand (e.g.\ classification, regression).

\subsubsection{Output layers}

For classification, it is common to use an activation function that makes it possible to interpret the outputs as probabilities.
A common choice is the softmax function, which normalises the output to sum to 1:
\begin{equation}
\mathrm{softmax}(\mat z)_i = \frac{e^{z_i}}{\sum_j e^{z_j}}.
\end{equation}
For multi-label classification one can use the logistic function, defined as
\begin{equation} \label{eq:logistic}
\sigma(\mat z)_i = \frac{1}{1 + e^{-z_i}}.
\end{equation}

When doing regression on the other hand, it is common to simply use a linear activation function:
\begin{equation}
 g(\mat z) = \mat z.
\end{equation}

\subsubsection{Hidden layers}

It can be shown that a stacked model with linear activations in the hidden layers is equivalent to a shallow model with no hidden layers.
Thus, it is vital that the activation function used for the hidden layers be non-linear.
However, it can be advantageous to use a \emph{mostly linear} activation function, as the derivative of such a function will be constant or nearly constant for a large set of inputs, which allows the network to be trained more efficiently.

Examples of mostly linear activation functions that have been shown to outperform other activation functions in many contexts include the rectified linear unit \parencite{glorot2011deep}:
\begin{equation}
\mathrm{ReLU}(\mat z) = \max(\mat 0, \mat z)
\end{equation}
and the maxout unit \parencite{goodfellow2013maxout}:
\begin{equation}
\mathrm{maxout}(\mat y^{l-1}) = \max(\mat y^{l-1} \mat W^l + \mat b^l, \mat y^{l-1} \mat V^l + \mat c^l)
\end{equation}
where $\max$ is performed element-wise, i.e.\ $\max(\mat u, \mat v)_i = \max(u_i, v_i)$.
Note that the input to maxout is the output of the previous layer rather than a weighted sum; it is a generalisation of the ReLU, keeping multiple (not necessarily limited to only two) sets of weight and bias values.

% \todo[inline]{add source (german??) or remove as digression}
% \todo[inline]{weights play a large role too...}
% Historically it has also been common to use sigmoidal (S-shaped) functions such as the logistic function or $\tanh$ for hidden layers as well, as a way to approximate the spiking behaviour of biological neurons.
% However, a major issue with a sigmoidal activation function is that, in addition to being comparatively slow to compute, the derivative of the function quickly falls towards 0 as the function saturates for large positive and negative input values, which tends to occurr as training progresses.
% This causes training of the network to slow down, especially in networks with many hidden layers where the derivatives of multiple activation functions are multiplied, in a phenomenon known as the ``vanishing gradient'' problem.

\subsection{Training neural networks}

In order to produce any useful results, the parameters (weights and biases) $\mat \theta$ of the network need to be tuned.
This tuning is referred to as \emph{training} the network, and is performed by minimising some loss function (also known as a cost function or objective function) $L(\mat \theta; \mat x, \mat y)$ where $\mat x$ is the input to the network, and $\mat y$ is the \emph{expected} output of the network.
If $\hat{\mat y} = f(\mat x; \mat \theta)$ is the actual output of the network, $L(\mat x, \mat y; \boldsymbol \theta)$ defines some measure of how dissimilar $\hat{\mat y}$ and $\mat y$ are.

For classification, the most common loss function is the cross-entropy between $\mat y$ and $\mat x$, defined as
\begin{equation}
 L_{\mathrm{CE}}(\mat \theta; \mat x, \mat y) = -\sum_{j=1}^m y_j \log \hat y_j.
\end{equation}
For single-class classification such that one output is 1 and all others 0, this simplifies to
\begin{equation}
 L_{\mathrm{CE}}(\mat \theta; \mat x, \mat y) = -\log \hat y_j
\end{equation}
for the $j$ such that $y_j = 1$.
Minimising the cross-entropy between $\mat y$ and $\mat x$ is equivalent to minimising the Kullback--Leibler divergence between the points, making it appropriate as a loss function when interpreting the network output as a probability distribution.

Regression often makes use of the mean squared error instead:
\begin{equation}
L_{\mathrm{MSE}}(\mat \theta; \mat x, \mat y) = \lVert\hat {\mat y} - \mat y\rVert_2^2.
\end{equation}
Autoencoders, which attempt to find a low-dimensional representation of the input data from which the data can be reconstructed, use $L_{\mathrm{MSE}}(\mat \theta; \mat x, \mat x)$, called the reconstruction error, as the loss function.

Once the loss function is defined, the network can be trained by iteratively modifying the network parameters through gradient descent.
Usually a special form of gradient descent called minibatch gradient descent is used, where the network is presented with a small subset of the examples at each iteration, and the gradient is averaged over the presented examples.
Let $B = \{i_1, i_2, \dots, i_K\}$ be $K$ indices forming one minibatch.
We can then state the training procedure as
\begin{equation}
\boldsymbol \theta^{t+1} \gets \boldsymbol \theta^t - \eta\frac{\partial}{\partial \boldsymbol \theta}\left(\frac{1}{|B|} \sum_{i \in B} L(\mat \theta^t; \mat x_i, \mat y_i)\right)
\end{equation}
where $\eta$ is the ``learning rate''.
Tuning the learning rate is important, as a high learning rate means faster learning, but setting it too high may prevent the training from converging.

By exploiting the layered structure of feedforward neural networks, it is possible to calculate the gradient efficiently in a recursive manner.
This algorithm is commonly referred to as the backpropagation algorithm \parencite{rumelhart1986learning}.
