% Copyright (C) 2016  Arvid Fahlstr√∂m Myrman
%
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License as published by
% the Free Software Foundation; either version 2 of the License, or
% (at your option) any later version.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
%
% You should have received a copy of the GNU General Public License along
% with this program; if not, write to the Free Software Foundation, Inc.,
% 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

\section{Related work}
\label{ch:related-work}

This \namecref{ch:related-work} provides a brief overview of recent research on unsupervised acoustic modelling.
The approaches discussed here can broadly be divided into two categories: frame-based approaches that infer the acoustic model directly from the speech frames, and term discovery-based approaches that first segment the speech into syllable- or word-like fragments, and afterwards try break these fragments into smaller subword units.
See \cref{ch:theory} for a more detailed description of some of the concepts mentioned here.

\subsection{Frame-based approaches}

While on an abstract level words and sentences are composed of a sequence of discrete speech sounds, on an acoustic level speech is a continuous signal, with smooth transitions between sounds.
In order to more easily analyse speech we therefore generally segment the speech signal into a sequence of short frames of equal size.

As an individual speech frame only makes up a fraction of a complete speech sound, it is natural to model the speech using a model that can capture time dependencies, such as a hidden Markov model (HMM), rather than attempt to cluster the speech frames directly.
One issue with this approach, however, is that the number of possible states (i.e.\ subword units) is unknown a priori.

\textcite{varadarajan2008unsupervised} tackle this problem by first defining a one-state HMM, and then iteratively splitting and merging states as needed to account for the data according to a heuristic.
Training stops once the size of the HMM reaches a threshold.
After training, each state in the HMM can be thought to correspond to some allophone (context-dependent variant realisation) of a phoneme.
It should be noted, however, that in order to interpret a given state sequence as a single phoneme, \citeauthor{varadarajan2008unsupervised} train a separate model using labelled speech to perform this mapping.
The method is thus not fully unsupervised.

\textcite{lee2012nonparametric} take a fully probabilistic approach, defining a model that jointly performs segmentation and acoustic modelling.
An infinite mixture model of three-state hidden Markov model-Gaussian mixture models (HMM-GMMs) modelling subword units is defined using the Dirichlet process, and latent variables representing segment boundaries are introduced.
The data can be thought to be generated by repeatedly sampling an HMM to model a segment, sampling a path through the HMM, and for each state in the path sampling a feature vector from the corresponding GMM.
The probability of transitioning from one unit to another is thus not modelled.
Inference of the model is done using Gibbs sampling.

\textcite{siu2014unsupervised} use an HMM of a more classic form to model the data.
An initial transcription of the data in terms of state labels is first generated in an unsupervised manner using a segmental GMM (SGMM).
The HMM is then trained by iteratively updating the parameters of the model keeping the transcription fixed, and estimating the most likely transcription keeping the parameters fixed.
Note that the number of allowed states are here defined in advance.
$n$-gram statistics are then collected from the transcription and used for tasks such as unsupervised keyword discovery.

Diverging from previous approaches using temporal models, \textcite{chen2015parallel} perform standard clustering of speech frames using an infinite Gaussian mixture model.
After training, the speech frames are represented as posteriorgrams, which have been shown to be more speaker invariant than other features such as mel frequency cepstral coefficients \parencite{zhang2010towards}.
Despite the simple approach, this turned out to be the overall best-performing model in the first track of the 2015 Zero Resource Speech Challenge \parencite{versteegh2016zero}.
\textcite{heck2016unsupervised} later further improved on the model by performing clustering in two stages, with an intermediate supervised dimensionality reduction step using the clusters derived from the first clustering step as target classes.

\textcite{synnaeve2016temporal} use a siamese network to create an embedding where speech frames close to each other are considered to belong to the same subword unit, while distant speech frames are said to differ.
A siamese network is a feedforward neural network that takes two inputs and adjusts its parameters to either maximise or minimise the similarity of the corresponding outputs \parencite{bromley1994signature}.

\subsection{Term discovery-based approaches}

An alternative to the frame-based approach is to first find longer pairs of word-like fragments using unsupervised term discovery (UTD).
These pairs can serve as constraints, since if both fragments in a pair correspond to the same word, then logically the sounds that make up the fragment pairs should be the same as well.
The rationale is that while at the frame level the same speech sound can seem quite different between different speakers or even different realisations of the sound by the same speaker, patterns over a longer duration of time are easier to identify; this idea is illustrated in \textcite{jansen2013weak}.

The methods discussed here generally use UTD systems based on the segmental dynamic time warping (S-DTW) developed by \textcite{park2008unsupervised}.
S-DTW works by repeatedly performing dynamic time warping (DTW) on two audio streams while constraining the maximum amount of warping allowed, each time changing the starting point of the DTW in both streams.
This yields a set of alignments, from which the stretches of lowest average dissimilarity in each alignment can be extracted.
Unfortunately, this approach is inherently $O(n^2)$ in time.
To remedy this, \textcite{jansen2011efficient} introduced an approximate version that uses binary approximations of the feature vectors to perform the calculations in $O(n \log n)$ time using sparse similarity matrices; this system also serves as the baseline for the second track of the Zero Resource Speech Challenge \parencite{versteegh2015zero}.

\textcite{jansen2011towards} describe a method for finding subword units by clustering HMM states.
The method assumes the availability of clusters corresponding to words, where each cluster contains multiple examples of the word in question in the form of audio.
For each word, an HMM is trained on all the corresponding examples, the number of states in the model being set to a number proportional to the average duration of the word.
The states from each HMM are then collected and clustered based on the similarity of their distributions, forming clusters that hopefully correspond to subword units.

\textcite{jansen2013weak} take somewhat of an inverse approach, starting by clustering the whole data on a frame level, with the assumption that each cluster will tend to correspond to some speaker- or context-dependent subword unit.
They then look at pairs of word-like fragments known to be of the same type and calculate how often clusters tend to co-occurr.
The clusters are then partitioned so that clusters that co-occurr often are placed in the same partition.

\textcite{synnaeve2014phonetics} introduce a neural network referred to as the ABnet, based on siamese networks \parencite{bromley1994signature}.
The network takes a pair of speech frames as input, and adjusts its parameters so that the outputs are collinear if the inputs are known to correspond to the same subword unit, and orthogonal otherwise, using a cosine-based loss function.
\textcite{thiolliere2015hybrid} made use of this approach in the Zero Resource Speech Challenge, also incorporating unsupervised term discovery so as to make the whole process unsupervised, yielding competitive results \parencite{versteegh2016zero}.
\textcite{zeghidour2016deep} experiment with supplying the ABnet with scattering spectrum features instead of filter bank features, showing that with the right features, a shallow architecture may outperform a deep architecture, especially when the amount of available data is low.

\textcite{kamper2015unsupervised} use an autoencoder-like structure, where a neural network is trained to ``reconstruct'' a frame given another frame known to be of the same type.
\textcite{renshaw2015comparison} used this architecture in the Zero Resource Speech Challenge, albeit with a deeper decoder.
