% Copyright (C) 2016  Arvid FahlstrÃ¶m Myrman
%
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License as published by
% the Free Software Foundation; either version 2 of the License, or
% (at your option) any later version.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
%
% You should have received a copy of the GNU General Public License along
% with this program; if not, write to the Free Software Foundation, Inc.,
% 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

% \begin{figure}
%   \centering
%   Sound + transcription + posteriorgrams
% 
%   \caption{\label{fig:postspec}Posteriorgram spectrum}
% \end{figure}

\chapter{Introduction}
\section{Background}
Automatic speech recognition (ASR) is generally framed as a supervised task, where both acoustic speech data and the corresponding transcription is available, and the problem is to develop a model that can mimic this mapping from speech to text.
However, developing such data is expensive, both in terms of time and money, as it involves painstakingly transcribing many hours of speech.
As a result, there is a notable lack of high-quality data for speech recognition for a majority of languages around the world.
An important question is thus whether it is possible to make use of untranscribed, or unlabelled, data to develop ASR for such low-resource languages.
Unsupervised learning in this manner may also provide insight into the linguistic structure of languages, or the language acquisition of infants.

In this work we focus in particular on one aspect of unsupervised learning of speech, namely the discovery of phonetic classes, i.e.\ the basic speech sounds that make up all words in a language.
While supervised speech recognition makes use of prior knowledge regarding what sounds are present in a language, in unsupervised acquisition of speech this knowledge is unavailable to us.
This means that not only do we need to be able to discover what sounds make up each word in a recording without the use of a transcription---we need to discover what sounds are even available in the language to begin with, as not all languages use the same sounds, nor are the same sounds contrastive in all languages.
This is compounded by speaker variation, where there can be significant differences between the pronunciation of sounds by individual speakers, or even by the same speaker depending on e.g.\ the context of the sound.
This variation makes approaches such as naive clustering ineffective, as many of the discovered sounds are likely to be highly speaker-specific.

Unsupervised speech acquisition is an area of active research.
One source of such research is the Zero Resource Speech Challenge \parencite{versteegh2015zero}, which was developed with the goal of finding linguistic units (track 1) or longer recurring word-like fragments (track 2) in speech.
Models are to be trained using only speech data, voice activity information, and speaker identity information.
The main goal of the first track of the challenge is to find robust representations of speech frames where sounds belonging to the same phonetic category are more similar than sounds belonging to different categories; this is also the approach we will take in this work.

\input{03-related-work.tex}

\section{This thesis}

\begin{figure}
  \centering
  \begin{tikzpicture}[outpos/.style={draw,rectangle},inpos/.style={draw,circle,thick},
    arr/.style={->,thick},yscale=0.8]
    \node[outpos] at (0,0) {$\Sigma$};
    \node[outpos] (out3) at (0,1) {$\Sigma$};
    \node[outpos] at (0,2) {$\Sigma$};
    \node[outpos] (out2) at (0,3) {$\Sigma$};
    \node[outpos] (out1) at (0,4) {$\Sigma$};
    
    \node[inpos,blue] (a1) at (-4, 3.5) {a};
    \node[inpos,red] (a2) at (-4.5, 0.5) {a};
    \node[inpos,blue] (k1) at (-3.4, 1.8) {k};
    \node[inpos,red] (k2) at (-2.5, 1.5) {k};
    \node[inpos] (k3) at (-2.5, 4.2) {k};
    \node[inpos] (m1) at (-1.8, 0.1) {m};
    
    \draw (a1) edge[arr] (out2) (a2) edge[arr,bend right=24] (out2)
          (k1) edge[arr] (out1) (k2) edge[arr] (out1) (k3) edge[arr] (out1)
	  (m1) edge[arr] (out3);
  \end{tikzpicture}

  \caption{\label{fig:mapping}An example of merging speaker-specific (coloured) clusters by mapping them to (a subset of) the available outputs.
  As we represent the input as a probability vector, each output is a simple sum of the input probabilities.}
\end{figure}

We take inspiration from two of the most successful approaches so far: the GMM clustering approach of \textcite{chen2015parallel} and the siamese network approach of \textcite{thiolliere2015hybrid}, described above.
We pose the question of whether it is possible to combine the two approaches in order to take advantage of both the full unlabelled data set, as in the former approach, and the smaller set of discovered fragments, as in the latter approach.
One way to do this is to first cluster the whole data set in a fully unsupervised manner, discovering latent classes from the data such that similar data points belong to the same latent class.
After extracting a latent representation of the data from the clustering model, we can then improve on this representation using speech fragment information.

For this work we consider in particular probabilistic models such as Gaussian mixture models and hidden Markov models.
These models express class membership in terms of probabilities, giving the posterior probability of each data point belonging to one of the latent classes.
By representing each data point as a vector of these posterior probabilities, called a posteriorgram, the task of improving this representation can be framed as one of merging, or partitioning, the latent classes appropriately.
Latent classes discovered through unsupervised training will generally be highly speaker-specific.
However, by merging the classes using weak supervision in the form of speech fragment pairs, we can construct classes that are more speaker invariant.

A partitioning of classes can be viewed as finding a function which maps the original set of classes to a class set of lower cardinality, but finding this function is a discrete problem which is difficult to optimise for.
However, a benefit of posteriorgrams is that the probability of an output class can be described as a simple sum of the probabilities of the input classes that map to the output in question, as illustrated in \cref{fig:mapping}.
This means that the mapping function can be approximated using a continuous linear model which can be optimised through standard gradient descent.
A linear model also has the benefit of being more interpretable than deep networks such as that of \textcite{thiolliere2015hybrid}.
While the approach of partitioning posteriorgrams is very reminiscent of \textcite{jansen2013weak}, the major difference is that in place of direct clustering of classes, we are instead trying to maximise the similarity/dissimilarity between pairs of speech fragments, which only indirectly results in a partitioning of the classes.

\begin{figure}
  \centering
  \begin{tikzpicture}
    %\begin{axis}[enlargelimits=false,axis on top,width=8cm,height=4cm,ylabel={Frequency (\si\Hz)},xlabel={Time (\si\s)}]
    %\end{axis}

% speaker 1
% s3102b 146.518752 147.111647 questions
%   146.518752  122 ng
%   146.596735  122 k
%   146.624258  122 w
%   146.693066  122 ah
%   146.760727  122 s; *
%   146.816920  122 ch
%   147.021050  122 en
%   147.111647  122 z

% speaker 2
% s2302a 106.778375 107.628939 questions
%   106.778375  122 ng
%   106.883302  122 k
%   106.921215  122 w
%   106.985553  122 eh
%   107.098375  122 s ; *
%   107.168375  122 ch
%   107.246360  122 ah
%   107.348375  122 n
%   107.628939  122 z
    \begin{groupplot}[group style={group size=2 by 3, horizontal sep=1.3cm, vertical sep=0.4cm},
      enlargelimits=false,width=5cm,height=4.5cm,axis on top,xtick=\empty]
      \nextgroupplot[ylabel={Frequency (\si\Hz)},title={Female speaker}]
      \addplot graphics[xmin=146.52,xmax=147.11,ymin=0,ymax=3125] {data/spectrum-speaker-1-crop.pdf};
      \coordinate (kw1t) at (axis cs:146.59,\pgfkeysvalueof{/pgfplots/ymax});
      \coordinate (we1t) at (axis cs:146.62,\pgfkeysvalueof{/pgfplots/ymax});
      \coordinate (es1t) at (axis cs:146.69,\pgfkeysvalueof{/pgfplots/ymax});
      \coordinate (sc1t) at (axis cs:146.76,\pgfkeysvalueof{/pgfplots/ymax});
      \coordinate (cu1t) at (axis cs:146.82,\pgfkeysvalueof{/pgfplots/ymax});
      \coordinate (un1t) at (axis cs:147.02,\pgfkeysvalueof{/pgfplots/ymax});

      \nextgroupplot[title={Male speaker}]
      \addplot graphics[xmin=106.78,xmax=107.63,ymin=0,ymax=3125] {data/spectrum-speaker-2-crop.pdf};
      \coordinate (kw2t) at (axis cs:106.88,\pgfkeysvalueof{/pgfplots/ymax});
      \coordinate (we2t) at (axis cs:106.92,\pgfkeysvalueof{/pgfplots/ymax});
      \coordinate (es2t) at (axis cs:106.99,\pgfkeysvalueof{/pgfplots/ymax});
      \coordinate (sc2t) at (axis cs:107.10,\pgfkeysvalueof{/pgfplots/ymax});
      \coordinate (cu2t) at (axis cs:107.17,\pgfkeysvalueof{/pgfplots/ymax});
      \coordinate (un2t) at (axis cs:107.35,\pgfkeysvalueof{/pgfplots/ymax});
      
      \nextgroupplot[ylabel={GMM component}]
      \addplot graphics[xmin=146.52,xmax=147.11,ymin=1,ymax=1024] {data/gmm-posteriorgrams-speaker-1-crop.pdf};
      
      \nextgroupplot
      \addplot graphics[xmin=106.78,xmax=107.63,ymin=1,ymax=1024] {data/gmm-posteriorgrams-speaker-2-crop.pdf};
      
      \nextgroupplot[ylabel={Model output},xlabel={Time}]
      \addplot graphics[xmin=146.52,xmax=147.11,ymin=1,ymax=33] {data/reduced-posteriorgrams-speaker-1-crop.pdf};
      \coordinate (kw1b) at (axis cs:146.59,\pgfkeysvalueof{/pgfplots/ymin});
      \coordinate (we1b) at (axis cs:146.62,\pgfkeysvalueof{/pgfplots/ymin});
      \coordinate (es1b) at (axis cs:146.69,\pgfkeysvalueof{/pgfplots/ymin});
      \coordinate (sc1b) at (axis cs:146.76,\pgfkeysvalueof{/pgfplots/ymin});
      \coordinate (cu1b) at (axis cs:146.82,\pgfkeysvalueof{/pgfplots/ymin});
      \coordinate (un1b) at (axis cs:147.02,\pgfkeysvalueof{/pgfplots/ymin});
      
      \nextgroupplot[xlabel={Time}]
      \addplot graphics[xmin=106.78,xmax=107.63,ymin=1,ymax=33] {data/reduced-posteriorgrams-speaker-2-crop.pdf};
      \coordinate (kw2b) at (axis cs:106.88,\pgfkeysvalueof{/pgfplots/ymin});
      \coordinate (we2b) at (axis cs:106.92,\pgfkeysvalueof{/pgfplots/ymin});
      \coordinate (es2b) at (axis cs:106.99,\pgfkeysvalueof{/pgfplots/ymin});
      \coordinate (sc2b) at (axis cs:107.10,\pgfkeysvalueof{/pgfplots/ymin});
      \coordinate (cu2b) at (axis cs:107.17,\pgfkeysvalueof{/pgfplots/ymin});
      \coordinate (un2b) at (axis cs:107.35,\pgfkeysvalueof{/pgfplots/ymin});

  \end{groupplot}
  \draw [thin,white] (kw1t) -- (kw1b);
  \draw [thin,white] (we1t) -- (we1b);
  \draw [thin,white] (es1t) -- (es1b);
  \draw [thin,white] (sc1t) -- (sc1b);
  \draw [thin,white] (cu1t) -- (cu1b);
  \draw [thin,white] (un1t) -- (un1b);
  
  \draw [thin,white] (kw2t) -- (kw2b);
  \draw [thin,white] (we2t) -- (we2b);
  \draw [thin,white] (es2t) -- (es2b);
  \draw [thin,white] (sc2t) -- (sc2b);
  \draw [thin,white] (cu2t) -- (cu2b);
  \draw [thin,white] (un2t) -- (un2b);
  \end{tikzpicture}

  \caption{\label{fig:model-output}Example of using the proposed model to process utterances of the word ``question'' from two different speakers.
  First the energy spectrum is calculated for each speech frame (top).
  After preprocessing the spectrum is then fed to a Gaussian mixture model from which posterior probabilities for each latent class is extracted (middle).
  Last, the probability vectors from the GMM are reduced in size using the proposed model (bottom).
  The white crosses mark the most probable class state for each frame.
  The white vertical lines mark boundaries between speech sounds.
  The example is meant to be illustrative and does not represent the overall performance of the model.}
\end{figure}

Using such a linear model, we hope to use speech fragment information to improve on posteriorgrams obtained from an unsupervised probabilistic model.
In particular we hope to use the model to construct more speaker-invariant posteriorgrams which can be used to discriminate between different linguistic units in a robust manner.
An example of using the model can be seen in \cref{fig:model-output}; the model takes high-dimensional posteriorgrams describing a probability distribution over latent variables (classes) from a Gaussian mixture model, and outputs low-dimensional posteriorgrams which are more speaker invariant.
We evaluate the model using the minimal-pair ABX task used for evaluation in the first track of the Zero Resource Speech Challenge.
The minimal-pair ABX task aims to evaluate the quality of different representations of speech by ensuring that two utterances of the same word are more similar to each other than to a distinct but similar word.
