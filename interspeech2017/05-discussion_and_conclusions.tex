%\section{Discussion}
%We have seen that the model is indeed able to improve on the input posteriors.
%In particular, the model improves the across-speaker performance, with little to no degradation of the within-speaker performance.
%However, the Jensen-Shannon loss function used is shown to perform worse in general than coscos$^2$, possibly as a result of being more sensitive to the balancing of the same-class and different-class losses.
%This can be explained by the fact that the Jensen-Shannon divergence is not directly interpretable---for instance, it is not clear that a same-class loss of $0.1$ is as good as a different-class loss of $0.9$.
%On the other hand, the cosine difference is more readily (geometrically) interpretable.
%
%However, the model itself does come with a number of advantages over deep models.
%The linear nature of the model means that the number of parameters is small, making the model fast and easy to train, and robust against overfitting.
%This is especially the case when imposing the entropy penalty, which can be seen as restricting the capacity of the model.
%The sparsity of the model additionally makes it more interpretable, providing insight into how exactly the input classes are mapped to the output.
%The model is also readily convertible into an exact surjection, resulting in a proper partition of the input classes.
%
%Another feature of the model is that it can take any kind of probability distribution as input, with the only requirement being that the underlying true classes are disentangled in the input.
%This makes it possible to use any kind of probabilistic model that admits a discrete posterior distribution over classes or states, including e.g.\ Gaussian mixture models or hidden Markov models.
%The resulting posteriorgrams can then be improved further by using the model to find a mapping to a smaller number of classes.
%
%One important question is how sensitive the model is to the dimensionality of the input.
%As the model requires evidence in terms of same-class or different-class pairs to know where to map each input class, a lack of evidence can result in classes being incorrectly merged (or unmerged, conversely).
%As the input size grows, the amount of evidence required grows as well.
%As such, it is advisable to choose an input size that reflects the amount of evidence available.
%This may explain the poor performance of the model on the Xitsonga data set, as far fewer speech fragments were found for Xitsonga than for English.

\section{Conclusions}
A linear model for approximate partitioning of posteriorgrams was introduced.
Using posteriorgrams from a Gaussian mixture model trained on MFCCs as a proof of concept, the model was shown to improve the across-speaker performance, with competitive results for the English data set.
While the better-performing versions of the model depends on two hyperparameters, the hyperparameter search is alleviated somewhat by ease of training the linear model.
Additionally, the entropy penalty was shown to be easy to optimise for, allowing a small value for the corresponding hyperparameter.
The silhouette cluster separation measure was shown to be indicative of ABX performance, enabling hyperparameter search without making use of the gold transcription.

The resulting model is sparse and easily interpretable.
However, the Jensen-Shannon loss function used is sensitive to the balancing of the same-class and different-class losses, making it particularly unsuitable for deep architectures.

\subsection{Future work}
A natural extension of this work is to use different probabilistic models to generate the posteriorgrams, and see how this affects the performance of the model.
For instance, would the model be able to improve on the posteriorgrams generated by the model of \textcite{chen2015parallel}?
Of interest are also models that directly model time dependencies, such as hidden Markov models.

The model as presented here can be seen as a kind of radial basis function (RBF) network, where the RBF units (i.e.\ the Gaussian mixture model) are trained on the complete data set, while the output weights are trained using gradient descent on the fragment pair data.
As such it might be interesting to see whether joint training of both the input clusters and the linear mapping by treating the model as a single RBF network would lead to any improvements.

Finally, as we have seen the Jensen-Shannon loss needs to reweighted in order to properly balance the same-class and different-class losses.
It is thus desirable to find an alternative loss function suitable for probability distributions, for which the losses are naturally more balanced.



%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
