\section{Conclusions}
A linear model for partitioning of posteriorgrams was introduced and applied to unsupervised learning of linguistic units in speech.
The model was shown to be able to reduce the dimensionality of GMM posteriorgrams from 1024 to below 40, while simultaneously improving the across-speaker robustness.
The model does not depend on the GMM, however, as it is able to take posteriorgrams generated from any probabilistic model as input, the only requirement being that the underlying true classes are disentangled in the input representation.

While the model depends on two hyperparameters, the hyperparameter search is alleviated somewhat by the ease of training the linear model.
Additionally, the entropy penalty was shown to be easy to optimise for.
The silhouette was shown to be indicative of ABX performance, enabling hyperparameter search without making use of the gold transcription.

The resulting model is sparse, easily interpretable, and robust to overfitting as a result of the low number of parameters and the regularisation imposed by the entropy penalty.
This entropy penalty also results in only a subset of the outputs being used, making the model insensitive to the total number of available outputs.
However, the Jensen-Shannon loss function used is sensitive to the balancing of the same-class and different-class losses, making it particularly unsuitable for deep architectures.

We believe that the disparity in performance between English and Xitsonga may be due to the lower number of speech fragment pairs obtained through unsupervised term discovery for Xitsonga.
Further investigation is needed to assess why our model is more adversely affected by this than deeper models.


%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
