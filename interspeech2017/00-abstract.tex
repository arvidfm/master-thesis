\textcite{jansen2013weak} have previously demonstrated success in learning unsupervised models of speech by partitioning models inferred from unlabelled data, using speech fragments discovered through unsupervised term discovery (UTD) as top-down constraints.
We propose an alternative partitioning strategy: Viewing a partitioning as a mapping from a set of input classes to a smaller set of output classes, we relax the problem by considering a linear transformation of posteriorgrams to a lower-dimensional output.
This allows us to train a sparse linear siamese model using gradient descent, where we are able to make use of both same-category and different-category speech fragment examples.
Post training, the model is readily converted into an exact partitioning of the posteriorgrams.

We evaluate the model on the minimal-pair ABX task in the context of the Zero Resource Speech Challenge, and are able to demonstrate that, given posteriorgrams generated from a simple Gaussian mixture model as input, the model outputs naturally low-dimensional posteriorgrams that are more robust to speaker variance.
This suggests that the model may be viable as a post-processing step to improve the output of probabilistic models in general.

%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
