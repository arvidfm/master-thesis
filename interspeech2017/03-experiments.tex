\section{Experiments}
\label{sec:experiments}

\subsection{Data}
To test our method we use the data from the 2015 Zero Resource Speech Challenge.
The Challenge makes use of two corpora: The Buckeye corpus of conersational English \parencite{buckeyecorpus} and the NCHLT speech corpus of read Xitsonga \parencite{barnard2014nchlt}.
For the challenge only a subset of the data is used, consisting of 12 speakers for a total of 5 hours of data for the Buckeye corpus, and 24 speakers for a total of 2.5 hours of data for the NCHLT Xitsonga corpus.
Additionally provided is voice activity information indicating segments containing clean speech, as well as labels indicating the identity of the speaker.

MFCCs features were extracted from the data using a frame window length \SI{25}{\ms} which was shifted \SI{10}{\ms} for each frame, an FFT resolution of 512 frequency steps, and 40 mel-spaced triangular filter banks.
13 coefficients with both delta and delta-delta features were used.
The MFCCs corresponding to segments with voice activity were clustered using an implementation of a Gaussian mixture model (GMM) provided by scikit-learn \parencite{scikit-learn}.
The GMM was trained using the expectation maximisation algorithm, using $M = 1024$ Gaussians with diagonal covariance matrices, for a maximum of 200 iterations.
After training posteriorgrams are calculated for each frame.

The unsupervised term discovery yielded 6512 fragments and 3149 clusters for the Buckeye corpus, and 3582 fragments and 1782 clusters for the NCHLT Xitsonga corpus\footnote{The cluster files used for this work were generously provided by Roland Thiolli√®re and Aren Jansen.}.
70\% of the same-class and different-class fragment pairs were used for training, with the remaining pairs used for validation to determine when to interrupt the training of the models.

%\subsection{Unsupervised term discovery}
%\label{sec:utd}

% Pairs of similar speech fragments were discovered using the system developed by \textcite{jansen2011efficient}, which serves as a baseline for the second track of the Zero Resource Speech Challenge.
% The system works by calculating the approximate cosine similarity between pairs of frames of two input audio segments, based on discretised random projections of PLP features.
% For efficiency only frames found using an approximate nearest neighbour search are compared, yielding a sparse similarity matrix.
% Stretches of similar frames are then found by searching for diagonals in the similarity matrix, which which are then aligned using dynamic time warping (DTW).
% Pairs of segments with a DTW score above a certain threshold are kept and clustered based on pairwise DTW similarity, resulting in a set of clusters of speech segments, or fragments, thought to be of the same class (e.g.\ word).

% For each cluster every possible pair of fragments was extracted from the collection of posteriorgrams retrieved from the GMM and aligned using DTW, yielding pairs of speech frames belonging to the same class.
% Let $K$ be the total number of pairs of fragments aligned.
% To generate a set of pairs of frames belonging to different classes, $K$ fragments were sampled uniformly from the full collection of fragments.
% For each such fragments, another fragment was sampled uniformly from the fragments belonging to a different cluster.
% When sampling fragments belonging to a different cluster, the sampling was performed using only either fragments spoken by the same speaker, or fragments spoken by a different speaker, with a probability corresponding to the ratio of same-speaker to different-speaker pairs among the same-class fragment pairs.
% The different-class fragment pairs were aligned by simply truncating the longer fragment.

\subsection{Model implementation}
\todo[inline]{mention server specification? cpu/ram/gpu}

We used $D = 64$ outputs for all models.
The models were trained using AdaMax \parencite{kingma2014adam} with the recommended default parameters $\alpha = 0.002$, $\beta_1 = 0.9$ and $\beta_2 = 0.999$.
All frames used for training were shuffled once at the start of training, and a minibatch size of 1000 frames was used.
The models were trained until no improvement had been observed on a held-out validation set for 15 epochs, where one epoch is defined as one complete scan over the training data.

All network models were implemented in Python 3.5 using Theano \parencite{theano} for automatic differentiation and GPU acceleration, librosa \parencite{librosa} for feature extraction, scikit-learn \parencite{scikit-learn} for various utilities, and numba \parencite{numba} for accelerating various code, in particular dynamic time warping.

\subsection{Tuning the hyperparameters}
\todo[inline]{mention somewhere how only a subset of the outputs are used by the model}
\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{groupplot}[
      group style={
	group size=2 by 1,
	horizontal sep=0.4cm,
	ylabels at=edge left,
	yticklabels at=edge left
      },
      ylabel=Divergence/entropy,
      xmin=0,xmax=0.3,ymin=0,ymax=0.7,
      width=0.38\columnwidth,height=2.5cm]
   \nextgroupplot[title=English,xlabel=Penalty ($\lambda$),
      %legend style={column sep=10pt},
      legend entries={JS loss,Same-class loss,Different-class loss,Normalised entropy},
      legend columns=2,
      legend to name=grouplegend,legend cell align=left
      ]
   \addplot table[x=lambda,y=js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=same-js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=diff-js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=entropy-v] {data/entropy_buckeye.txt};
   
   \nextgroupplot[title=Xitsonga,xlabel=Penalty ($\lambda$)]
   \addplot table[x=lambda,y=js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=same-js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=diff-js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=entropy-v] {data/entropy_xitsonga.txt};
  \end{groupplot}
  \node[yshift=-1.6cm] at ($(group c1r1.south)!.5!(group c2r1.south)$) {\ref{grouplegend}};
\end{tikzpicture}

\caption{\label{fig:entropy-penalty} Effect of varying the entropy penalty for the English (left) and Xitsonga (right) corpora.
The average entropy of the output distribution over the validation samples is shown along with the (root) Jensen-Shannon loss: Both the combined JS loss that is optimised for, and separately for same-class and different-class frame pairs.}
\end{figure}

$\lambda$ is chosen to be the smallest value that reduces the entropy to a satisfactory degree.
\Cref{fig:entropy-penalty} shows the resulting losses after convergence for the values of $\lambda$ tested, with $\alpha$ fixed to $1$.
We choose $\lambda = 0.1$ going forward, as no improvement of the entropy is seen for larger values.

\begin{figure}
 \centering
 \begin{tikzpicture}
   \pgfplotsset{set layers}
   \begin{axis}[
     xmin=0.8,xmax=4.2,
     ymax=0.17,
     axis y line*=left,
     xlabel=$\alpha$,
     ylabel=Silhouette,
     ylabel near ticks,
     yticklabel style={/pgf/number format/fixed},
     height=0.4\columnwidth,width=0.7\columnwidth,
     legend style={opacity=0.0}]%,
      %legend style={column sep=10pt},legend entries={Silhouette (English),Silhouette (Xitsonga)},legend cell align=left]
      \addplot table[x=alpha,y=sil-en] {data/silhouette.txt}; \label{sil1}
      \addlegendentry{Silhouette (English)}
   %\addplot+[mark=x] table[x=alpha,y=spread-en] {data/silhouette.txt};
   \addplot table[x=alpha,y=sil-ts] {data/silhouette.txt}; \label{sil2}
   \addlegendentry{Silhouette (Xitsonga)}
   %\addplot+[mark=x] table[x=alpha,y=spread-ts] {data/silhouette.txt};
   \end{axis}
   \begin{axis}[
     scale only axis,
     xmin=0.8,xmax=4.2,
     ymax=65,
     axis y line*=right,
     axis x line=none,
     ylabel=Outputs,
     ylabel near ticks,
     height=0.4\columnwidth,width=0.7\columnwidth,
     legend style={anchor=south,at={(0.5,1.02)}},
     legend columns=2]%,
      %legend style={column sep=10pt},legend entries={Silhouette (English),Silhouette (Xitsonga)},legend cell align=left]
   %\addplot+[mark=x] table[x=alpha,y=sil-en] {data/silhouette.txt};
   \addlegendimage{/pgfplots/refstyle=sil1}\addlegendentry{Silhouette (English)}
   \addlegendimage{/pgfplots/refstyle=sil2}\addlegendentry{Silhouette (Xitsonga)}
   \addplot+[dashed] table[x=alpha,y=spread-en] {data/silhouette.txt};
   \addlegendentry{Outputs (English)}
   %\addplot+[mark=x] table[x=alpha,y=sil-ts] {data/silhouette.txt};
   \addplot+[dashed] table[x=alpha,y=spread-ts] {data/silhouette.txt};
   \addlegendentry{Outputs (Xitsonga)}
   \end{axis}
 \end{tikzpicture}

 \caption{\label{fig:silhouette} Silhouette for different weightings of the same-class and different-class losses.
 Also shown is a heuristically calculated estimation of the number of outputs used by the model for different $\alpha$.}
\end{figure}

To find an optimal $\alpha$ we make use of the clusters discovered by the UTD system, choosing the $\alpha$ that maximises the separation of the clusters.
We use the silhouette \parencite{rousseeuw1987silhouettes} as a cluster separation measure, taking the distance between individual fragments to be the DTW score, with the symmetrised KL divergence as the frame-based distance.
The silhouette for different $\alpha$ with $\lambda$ fixed to $0.1$, calculated on a subset of 1000 clusters, can be seen in \cref{fig:silhouette}.
The optimal value is chosen as $\alpha = 1.5$ for both data sets.


\subsection{Discretising the model}
\label{sec:discrete}
As the resulting model is sparse, we can retrieve an exact surjection by discretising the model.
We do this by for each row in $\mat W$ setting the largest element to $1$ and the remaining elements to $0$.
Using the discretised model as a base, we additionally experiment with discretising the output distribution by setting the largest output to 1 and the rest to 0; this can be thought of as taking the argmax of the output distribution.

\subsection{Comparison with deep models}
\label{sec:deep}
To get an idea of how the JS loss performs in general, we build a deep network with two hidden layers of $500$ sigmoid units each, with $64$ softmax outputs.
The network is trained using the non-rebalanced JS loss.
As softmax outputs are naturally sparse, we do not enforce any entropy penalty.
For comparison we train the same architecture, albeit with sigmoid outputs instead, using the coscos$^2$ loss of \textcite{synnaeve2014phonetics}.
This is the architecture used by \textcite{thiolliere2015hybrid} in the 2015 Zero Resource Speech Challenge.

As input to both networks we use the log-scale outputs of 40 mel-scaled filter banks.
All other relevant parameters are the same as for the MFCCs calculated in \cref{sec:posteriorgrams}.
The filter bank outputs are normalised over the whole data set to have zero mean and unit variance for all dimensions.
Each frame is fed to the network with a context of 3 frames on both sides, for a total of 280 values used as input to the network.
All fragments are DTW aligned and sampled as in \cref{sec:utd}.

%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
