\section{Results}
\label{sec:results}

\subsection{ABX evaluation}

We evaluate the models discussed on the minimal-pair ABX task \parencite{schatz2013evaluating} using the toolkit provided for the Zero Resource Speech Challenge \parencite{versteegh2015zero}.
The results are shown in \cref{tab:abx}, along with the silhouette for each model.
The frame-based metric is chosen as the symmetrised Kullback-Leibler divergence (with the model output normalised as necessary), with the exception of the model with discretised output, which uses the cosine distance, which for one-hot vectors amounts to a distance of 0 for identical and 1 for non-identical vectors.

\todo[inline]{incorporate below paragraph into the text properly}
Within-speaker and across-speaker ABX scores as well as the silhouette for the different models for both the English and Xitsonga data sets.
   GMM posteriors is the posteriorgrams extracted from the 1024-component Gaussian mixture model; non-rebalanced is the original loss presented in \cref{eq:original-loss}; rebalanced is the alternative loss presented in \cref{eq:rebalanced} with ${\alpha = 1.5}$; discretised $\mat W$ and discretised output are the models presented in \cref{sec:discrete}; and the deep models are those presented in \cref{sec:deep}.
   The silhouette is calculated on a subset of 1000 clusters for each language.
   All shallow models are trained with an entropy penalty of $\lambda = 0.1$.

We can see that in general, the silhouette seems to be indicative of the relative performance of the models on the ABX task, with well-performing models having a higher silhouette score.
Among the shallow models, we see that rebalancing the same-class and different-class losses results in significant gains, with discretisation of the weights further improving the results.
Unsuprisingly however, discretising the output as well severely worsens the results, likely due to too much information being discarded.
The models were generally able to improve on the input posteriorgrams, especially for the across-speaker task.
The models performed significantly worse for Xitsonga than for English.

The deep model performs poorly when trained with the Jensen-Shannon loss, despite a similar architecture performing well when trained with the coscos$^2$ loss.
Inspecting the average output of the deep model over the English data set, we found that only 6 outputs are actually used by the model.
This suggests that the JS loss is more sensitive than the coscos$^2$ loss when it comes to balancing the same-class and different-class losses. \todo{too discussion-y?}
Note that we were unable to replicate the results of \textcite{thiolliere2015hybrid} using the coscos$^2$ loss.
All models performed significantly worse than the current state-of-the-art \parencite{heck2016unsupervised}.


%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
