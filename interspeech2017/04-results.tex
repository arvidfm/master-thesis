\section{Results}
\label{sec:results}

\subsection{Tuning the entropy penalty}
We train models using $\lambda \in \{0, 0.05, 0.1, \dots, 0.95\}$ for both the Buckeye and NCHLT Xitsonga corpora.
The final validation errors for each model are reported in \cref{fig:entropy-penalty}.
For both corpora, the entropy drops quickly even for small $\lambda$, suggesting that the entropy is relatively easy to optimise for.
As the entropy penalty is increased, the entropy itself does not decrease; however, the different-class JS loss decreases at the expense of the same-class JS loss. \todo{why?}
For future experiments, a penalty of $\lambda = 0.1$ is used.

\begin{figure*}
  \centering
  \begin{tikzpicture}
    \begin{groupplot}[group style={group size=2 by 1, horizontal sep=2cm},xmin=0,xmax=1,ymin=0,ymax=1,width=5cm,height=4cm]
      \nextgroupplot[title=English,xlabel=Penalty ($\lambda$),ylabel=Divergence/entropy,
      legend style={column sep=10pt},legend entries={JS loss,Same-class loss,Different-class loss,Normalised entropy},
      legend columns=2,legend to name=grouplegend,legend cell align=left]
   \addplot table[x=lambda,y=js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=same-js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=diff-js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=entropy-v] {data/entropy_buckeye.txt};
   
   \nextgroupplot[title=Xitsonga,xlabel=Penalty ($\lambda$),ylabel=Divergence/entropy]
   \addplot table[x=lambda,y=js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=same-js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=diff-js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=entropy-v] {data/entropy_xitsonga.txt};
  \end{groupplot}
  \node[yshift=1.6cm] at ($(group c1r1.north)!.5!(group c2r1.north)$) {\ref{grouplegend}};
\end{tikzpicture}

\caption{\label{fig:entropy-penalty} Effect of varying the entropy penalty for the English (left) and Xitsonga (right) corpora.
The average entropy of the output distribution over the validation samples is shown along with the (root) Jensen-Shannon loss: Both the combined JS loss that is optimised for, and separately for same-class and different-class frame pairs.}
\end{figure*}

\subsection{ABX evaluation}
\begin{table*}
 \centering
 \begin{tabular}{lrrrrrr} \toprule
   & \multicolumn{3}{c}{English} & \multicolumn{3}{c}{Xitsonga} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7}
    Model & Silhouette & Within & Across & Silhouette & Within & Across \\ \midrule
    GMM posteriors & 0.008 & 12.313 & 23.841 & 0.066 & 11.434 & 23.181 \\
    Non-rebalanced & 0.089 & 14.195 & 21.369 & 0.111 & 16.477 & 25.551 \\
    Rebalanced & 0.108 & 12.770 & 19.831 & 0.146 & 13.990 & 23.202 \\
    Discretised $\mat W$ & 0.124 & 12.013 & 19.261 & 0.170 & 12.702 & 21.888 \\
    Discretised output & 0.010 & 16.513 & 24.565 & 0.014 & 19.404 & 29.150 \\
    Deep JS & -0.370 & 22.376 & 28.233 & -0.320 & 18.190 & 24.759 \\
    Deep coscos$^2$ & 0.187 & 12.294 & 19.561 & 0.174 & 11.934 & 19.052 \\ \bottomrule
 \end{tabular}

 \caption{\label{tab:abx}Within-speaker and across-speaker ABX scores as well as the silhouette for the different models for both the English and Xitsonga data sets.
   GMM posteriors is the posteriorgrams extracted from the 1024-component Gaussian mixture model; non-rebalanced is the original loss presented in \cref{eq:original-loss}; rebalanced is the alternative loss presented in \cref{eq:rebalanced} with ${\alpha = 1.5}$; discretised $\mat W$ and discretised output are the models presented in \cref{sec:discrete}; and the deep models are those presented in \cref{sec:deep}.
   The silhouette is calculated on a subset of 1000 clusters for each language.
   All shallow models are trained with an entropy penalty of $\lambda = 0.1$.}
\end{table*}

We evaluate the models discussed on the minimal-pair ABX task \parencite{schatz2013evaluating}.
In the task we are presented with three speech fragments A, B and X, where A and B form minimal pairs, i.e.\ they only differ by a single phoneme.
The task is to decide which of either A or B belongs to the same category as X.
This is done by DTW-aligning A and B with X with respect to some underlying frame-based metric.
The fragment closest to X according to the DTW score is chosen.
The task takes two forms: within-speaker discriminability, where all fragments belong to the same speaker, and across-speaker discriminability, where A and B belong to one speaker while X belongs to another.

The models are evaluated using a evaluation toolkit provided for the Zero Resource Speech Challenge.
The results are shown in \cref{tab:abx}, along with the silhouette for each model.
The frame-based metric is chosen as the symmetrised Kullback-Leibler divergence (with the model output normalised as necessary), with the exception of the model with discretised output, which uses the cosine distance, which for one-hot vectors amounts to a distance of 0 for identical and 1 for non-identical vectors.

We can see that in general, the silhouette seems to be indicative of the relative performance on the ABX task.
Our suspicion that the number of outputs used were too few when using the Jensen-Shannon loss as originally stated is validated, with the rebalanced loss performing better for both English and Xitsonga.
The performance of the model with discretised weights further suggests that the basic premise of improving posteriorgrams by partitioning is a sound one.

The deep model performs poorly when trained with the Jensen-Shannon loss, despite the same architecture performing well when trained with the coscos$^2$ loss.
Inspecting the average output of the deep model over the English data set, we found that only 6 outputs are actually used by the model.
This suggests that the JS loss is more sensitive than the coscos$^2$ loss when it comes to balancing the same-class and different-class losses.


%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
