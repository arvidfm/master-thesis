\section{Method}
\label{sec:method}

The goal of our method is to find low-dimensional representations of posteriograms obtained by probabilistic unsupervised clustering such that the resulting representation is more invariant to speaker variation.
The dimensionality reduction is done by partitioning the latent classes corresponding to the posteriorgram components.
In order to do this in an unsupervised manner, we use information obtained by unsupervised term discovery (UTD) as in \parencite{jansen2011efficient}.
The result of the UTD is a set of clusters of speech fragments thought to be of the same category (e.g.\ word).
We then generate a set of same-class and different-class frame pairs, each frame represented as a posteriorgram, by sampling and aligning fragment pairs as in \parencite{thiolliere2015hybrid}.

We represent the input data as a set $\{(\mat x_i, \mat y_i)\}_{i=1}^N$ of $N$ pairs of $M$-dimensional posteriorgrams, along with a set of indicators $\{c_i\}_{i=1}^N$ such that $c_i$ is $1$ if $\mat x_i$ and $\mat y_i$ belong to the same category, and $0$ otherwise.
We wish to transform the input to $D$-dimensional posteriorgrams such that two inputs $\mat x_i$, $\mat y_i$ are close in output space if $c_i = 1$, and distant otherwise.
Our model is a simple linear transformation
\begin{equation}
 f(\mat x) = \mat x \mat W,\ \ \ \mat W \in \mathbb R^{M \times D}.
\end{equation}

In order to ensure that the output is a probability distribution, we need to constrain $\mat W$ so that each element is positive, and the elements of each row sum to $1$.
This is done by costructing the model as follows:
\begin{align}
  \mat V &\in \mathbb R^{M \times D} \\
  \mat{\widetilde W} &= |\mat V| \\
  \mat W &= \mat{\widetilde W} \oslash \left(\mat{\widetilde W} \mat 1_D \mat 1_D^T\right) \label{eq:normalize} \\
  f(\mat x; \mat V) &= \mat x \mat W
\end{align}
where $\mat 1_D$ is a column vector of $D$ ones and $\mat 1_D^T$ its transpose, $|\cdot|$ denotes the element-wise absolute value, and $\oslash$ denotes element-wise division.
Note that the function of \cref{eq:normalize} is to normalise the rows of $\mat W$ to sum to one.
This formulation makes it possible to optimise the model while ensuring that the constraints on $\mat W$ hold, by performing gradient descent with respect to $\mat V$.

To encourage the model to place points belonging to the same class close together in the output space, we use the siamese paradigm of \parencite{synnaeve2014phonetics,thiolliere2015hybrid}.
Let ${B_1 = \{i \in B : c_i = 1\}}$ be the subset of same-class pairs in the current minibatch, and ${B_0 = \{i \in B : c_i = 0\}}$ the subset of different-class pairs.
Additionally, let $\hat{\mat x}_i = f(\mat x_i; \mat V)$ and $\hat{\mat y}_i = f(\mat y_i; \mat V)$.
We then define the loss function over a minibatch $B$ as
\begin{multline}
  \label{eq:batch-loss}
  L_{\mathrm{JS}}(\mat V; B) = \frac{1}{(\alpha + 1)|B_1|}
  \sum_{i \in B_1} \overbrace{\sqrt{\mathrm{JS}(\hat{\mat x}_i || \hat{\mat y}_i)}}^{\mbox{same-class}} \\
  + \frac{\alpha}{(\alpha + 1)|B_0|}
  \sum_{i \in B_0} \underbrace{\left(1 - \sqrt{\mathrm{JS}(\hat{\mat x}_i || \hat{\mat y}_i)}\right)}_{\mbox{different-class}},
\end{multline}
where $\alpha$ is a hyperparameter determining how much to weigh the different-class loss over the same-class loss, and
$\mathrm{JS}(\mat x || \mat y)$ is the Jensen-Shannon (JS) divergence defined as
\begin{equation}
  \mathrm{JS}(\mat x || \mat y) = \frac{1}{2} \mathrm{KL}(\mat x || \mat m) + \frac{1}{2} \mathrm{KL}(\mat y || \mat m),
\end{equation}
where $\mathrm{KL}(\mat x || \mat y)$ is the Kullback-Leibler (KL) divergence, and $\mat m = (\mat x + \mat y) / 2$.
Thus, we attempt to minimise the JS divergence between same-class outputs, while maximising the divergence between different-class outputs.

The choice of a statistical distance as the loss function is motivated by the fact that the output of the model is a probability distribution.
The JS divergence was chosen over the KL divergence because it is symmetric, always defined, and bounded between 0 and 1 (when using the base-2 logarithm), making it more appropriate for maximising the divergence between different-class outputs.
Additionally, the square root of the JS divergence, used here, is a metric satisfying the triangle inequality \parencite{endres2003new}.

\subsection{Entropy penalty}
To ensure the interpretability of the output, we add a penalty term that attempts to minimise the entropy, i.e.\ the spread of the probability mass, in the output distribution.
We use the normalised entropy, defined as
\begin{equation}
  \hat H(\mat x) = -\frac{1}{\log_2 D} \sum_{i=1}^D x_i \log_2 x_i.
\end{equation}
The normalisation ensures that the entropy is always bounded between $0$ and $1$, regardless of the number of outputs $D$ of the model.
Over a minibatch $B$, the entropy loss function is given as
\begin{multline}
  L_{\mathrm{H}}(\mat V; B) = \\ \frac{1}{2|B|} \sum_{i \in B} \left(\hat H(f(\mat x_i; \mat V)) + \hat H(f(\mat y_i; \mat V))\right)
\end{multline}

The entropy penalty implicitly encourages sparsity in $\mat W$, as the only way to avoid spreading the probability mass across several outputs is for each row of $\mat W$ to only contain a single element close to $1$.
In summary, the complete loss over a minibatch $B$ is as follows:
\begin{equation}
  \label{eq:complete-loss}
  L(\mat V; B) = L_{\mathrm{JS}}(\mat V; B) + \lambda L_{\mathrm{H}}(\mat V; B)
\end{equation}
where $\lambda$ is a hyperparameter.

\begin{figure*}
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
  \begin{tikzpicture}
    \begin{groupplot}[
      group style={
	group size=2 by 1,
	horizontal sep=0.4cm,
	ylabels at=edge left,
	yticklabels at=edge left
      },
      ylabel=Divergence/entropy,
      ymin=0,ymax=0.7,
      enlarge x limits=true,
      width=0.38\columnwidth,height=2cm]
   \nextgroupplot[title=English,xlabel=Penalty ($\lambda$),
      legend entries={JS loss,Same-class loss,Different-class loss,Normalised entropy},
      legend columns=2,
      legend to name=grouplegend,legend cell align=left
      ]
   \addplot table[x=lambda,y=js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=same-js-v] {data/entropy_buckeye.txt};
   \addplot+[mark=triangle*,domain=0:0.3] table[x=lambda,y=diff-js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=entropy-v] {data/entropy_buckeye.txt};
   
   \nextgroupplot[title=Xitsonga,xlabel=Penalty ($\lambda$)]
   \addplot table[x=lambda,y=js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=same-js-v] {data/entropy_xitsonga.txt};
   \addplot+[mark=triangle*] table[x=lambda,y=diff-js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=entropy-v] {data/entropy_xitsonga.txt};
  \end{groupplot}
  \node[yshift=-1.4cm] at ($(group c1r1.south)!.5!(group c2r1.south)$) {\ref{grouplegend}};
  \end{tikzpicture}
  
  \caption{\label{fig:entropy-penalty}}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
 \begin{tikzpicture}
   \pgfplotsset{set layers}
   \begin{axis}[
     xmin=0.8,xmax=4.2,
     ymax=0.17,
     axis y line*=left,
     xlabel=$\alpha$,
     ylabel=Silhouette,
     ylabel near ticks,
     yticklabel style={/pgf/number format/fixed},
     height=0.35\columnwidth,width=0.7\columnwidth,
     legend style={opacity=0.0}]
      \addplot table[x=alpha,y=sil-en] {data/silhouette.txt}; \label{sil1}
      \addlegendentry{Silhouette (English)}
   \addplot table[x=alpha,y=sil-ts] {data/silhouette.txt}; \label{sil2}
   \addlegendentry{Silhouette (Xitsonga)}
   \end{axis}
   \begin{axis}[
     scale only axis,
     xmin=0.8,xmax=4.2,
     ymax=65,
     axis y line*=right,
     axis x line=none,
     ylabel=Outputs,
     ylabel near ticks,
     height=0.35\columnwidth,width=0.7\columnwidth,
     legend style={anchor=south,at={(0.5,1.02)}},
     legend columns=2]
   \addlegendimage{/pgfplots/refstyle=sil1}\addlegendentry{Silhouette (English)}
   \addlegendimage{/pgfplots/refstyle=sil2}\addlegendentry{Silhouette (Xitsonga)}
   \addplot+[dashed] table[x=alpha,y=spread-en] {data/silhouette.txt};
   \addlegendentry{Outputs (English)}
   \addplot+[dashed] table[x=alpha,y=spread-ts] {data/silhouette.txt};
   \addlegendentry{Outputs (Xitsonga)}
   \end{axis}
 \end{tikzpicture}
 \caption{\label{fig:silhouette}}
\end{subfigure}

 \caption{\textbf{Left:} Normalised entropy of the output distribution and the combined, same-class and different-class Jensen-Shannon losses, averaged over the validation set, as a function of the entropy penalty hyperparameter $\lambda$ for the English and Xitsonga data sets.
 \textbf{Right:} Silhouette for different weightings $\alpha$ of the same-class and different-class losses.
 Also shown is a heuristically calculated estimate of the number of outputs used by the model for different $\alpha$.}
\end{figure*}

\subsection{Binarising the model}
\label{sec:discrete}
As the resulting model is sparse, we can construct an exact partition of the input classes.
We do this by setting the largest element in each row in $\mat W$ to $1$, and the remaining elements to $0$, resulting in a binary $\mat W$.
An optional further processing step is to binarise the output distribution by setting the largest output to 1 and the rest to 0; this can be thought of as taking the argmax of the output distribution.

%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
