\section{Method}
\label{sec:method}
% Arvid: check that I am not getting this wrong

%\subsection{Unsupervised term discovery}
The goal of our method is to create a low-dimensional partition of posteriograms obtained by probabilistic unsupervised clustering such that the resulting representation is more invariant to speaker variation.
In order to do this in an unsupervised manner, we use information obtained by unsupervised term discovery (UTD) as in \parencite{jansen2011efficient}.
The result of the UTD is a set of clusters of speech fragments thought to be of the same category (e.g.\ word).
We then generate a set of same-class and different-class frame pairs, each frame represented as a posteriorgram, by sampling and aligning fragment pairs as in \parencite{thiolliere2015hybrid}.
%The system outputs clusters of word-like speech fragments thought to be of the same category.
%For each cluster we then use dynamic time warping to align every possible pair of fragments to generate a set of same-class frames, where each frame is represented as a posteriorgram.
%Different-class pairs are generated by sampling fragments from different clusters while preserving the ratio of same-speaker to different-speaker pairs as in \parencite{thiolliere2015hybrid}.

%\subsection{Model}

%\todo[inline]{intuitive explanation of what we want to do}
%The goal of our method is to merge acoustic clusters obtained by bottom-up unsupervised classification such that the resulting classes correspond more closely to phonemic units in the language.

%Given two input vectors $\mat x$ and $\mat y$, we wish to determine whether the vectors belong to the same category, or class.
%To do so, we project the vectors onto a space designed so that vectors belonging to the same class are close, while vectors belonging to different classes are distant.
%We consider in particular posteriorgrams---probability vectors $\mat p = (p_1, \dots, p_M)$ representing a posterior distribution over $M$ discrete values.
%The probabilities in $\mat p$ sum to $1$, with $p_i$ being the probability of the $i$th value; for instance, it could be the probability that a data point belongs to the $i$th latent class as inferred by a Gaussian mixture model.

%We assume that the $M$ input probabilities correspond to ``pseudo''-classes (e.g.\ allophones), where several pseudo-classes together describe a single ``true class'' (e.g.\ phonemes).
%Our goal is then to find a function $f$ that maps the $M$ pseudo-classes to a smaller set of $D$ output classes, where we take the probability of a single output class to be the sum of the probabilities of the pseudo-classes that map to the class in question.
%This can also be thought of as finding a partitioning of the input classes.
%It is vital that no two underlying classes are represented by the same input class, as our mapping would not be able to separate the classes.

We represent the input data as a set $\{(\mat x_i, \mat y_i)\}_{i=1}^N$ of $N$ pairs of $M$-dimensional posteriorgrams, along with a set of indicators $\{c_i\}_{i=1}^N$ such that $c_i$ is $1$ if $\mat x_i$ and $\mat y_i$ belong to the same category, and $0$ otherwise.
We wish to transform the input to $D$-dimensional posteriorgrams such that two inputs $\mat x_i$, $\mat y_i$ are close in output space if $c_i = 1$, and distant otherwise.
Our model is a simple linear transformation
\begin{equation}
 f(\mat x) = \mat x \mat W,\ \ \ \mat W \in \mathbb R^{M \times D}.
\end{equation}
%where $\mat W \in \mathbb R^{M \times D}$.

In order to ensure that the output is a probability distribution, we need to constrain $\mat W$ so that each element is positive, and the elements of each row sum to $1$.
This is done by costructing the model as follows:
\begin{align}
  \mat V &\in \mathbb R^{M \times D} \\
  \mat{\widetilde W} &= |\mat V| \\
  \mat W &= \mat{\widetilde W} \oslash \left(\mat{\widetilde W} \mat 1_D \mat 1_D^T\right) \label{eq:normalize} \\
  f(\mat x; \mat V) &= \mat x \mat W
\end{align}
where $\mat 1_D$ is a column vector of $D$ ones and $\mat 1_D^T$ its transpose, $|\cdot|$ denotes the element-wise absolute value, and $\oslash$ denotes element-wise division.
Note that the function of \cref{eq:normalize} is to normalise the rows of $\mat W$ to sum to one.
This formulation makes it possible to optimise the model while ensuring that the constraints on $\mat W$ hold, by performing gradient descent with respect to $\mat V$.

To encourage the model to place points belonging to the same class close together in the output space, we use the siamese paradigm of \parencite{synnaeve2014phonetics,thiolliere2015hybrid}.
Let ${B_1 = \{i \in B : c_i = 1\}}$ be the subset of same-class pairs in the current minibatch, and ${B_0 = \{i \in B : c_i = 0\}}$ the subset of different-class pairs.
Additionally, let $\hat{\mat x}_i = f(\mat x_i; \mat V)$ and $\hat{\mat y}_i = f(\mat y_i; \mat V)$.
We then define the loss function over a minibatch $B$ as
%\begin{multline}
%  \label{eq:batch-loss}
%  L_{\mathrm{JS}}(\mat V; B) = \frac{1}{(\alpha + 1)|B_1|} \sum_{i \in B_1} D_{\mathrm{same}}(\hat{\mat x}_i, \hat {\mat y}_i) \\ + \frac{\alpha}{(\alpha + 1)|B_0|} \sum_{i \in B_0} D_{\mathrm{diff}}(\hat {\mat x}_i, \hat {\mat y}_i),
%\end{multline}
\begin{multline}
  \label{eq:batch-loss}
  L_{\mathrm{JS}}(\mat V; B) = \frac{1}{(\alpha + 1)|B_1|}
  \sum_{i \in B_1} \overbrace{\sqrt{\mathrm{JS}(\hat{\mat x}_i || \hat{\mat y}_i)}}^{\mbox{same-class}} \\
  + \frac{\alpha}{(\alpha + 1)|B_0|}
  \sum_{i \in B_0} \underbrace{\left(1 - \sqrt{\mathrm{JS}(\hat{\mat x}_i || \hat{\mat y}_i)}\right)}_{\mbox{different-class}},
\end{multline}
where $\alpha$ is a hyperparameter determining how much to weigh the different-class loss over the same-class loss, and
%\begin{align} \label{eq:js-loss}
%  D_{\mathrm{same}}(\mat x, \mat y) &= \sqrt{\mathrm{JS}(\mat x || \mat y)} \\
%  D_{\mathrm{diff}}(\mat x, \mat y) &= 1 - \sqrt{\mathrm{JS}(\mat x || \mat y)},
%\end{align}
$\mathrm{JS}(\mat x || \mat y)$ is the Jensen-Shannon (JS) divergence defined as
\begin{equation}
  \mathrm{JS}(\mat x || \mat y) = \frac{1}{2} \mathrm{KL}(\mat x || \mat m) + \frac{1}{2} \mathrm{KL}(\mat y || \mat m),
\end{equation}
where $\mathrm{KL}(\mat x || \mat y)$ is the Kullback-Leibler (KL) divergence, and $\mat m = (\mat x + \mat y) / 2$.
Thus, we attempt to minimise the JS divergence between same-class outputs, while maximising the divergence between different-class outputs.

The choice of a statistical distance as the loss function is motivated by the fact that the output of the model is a probability distribution.
The JS divergence was chosen over the KL divergence because it is symmetric, always defined, and bounded between 0 and 1 (when using the base-2 logarithm), making it more appropriate for maximising the divergence between different-class outputs.
Additionally, the square root of the JS divergence, used here, is a metric satisfying the triangle inequality \parencite{endres2003new}.

\subsection{Entropy penalty}
To ensure the interpretability of the output, we add a penalty term that attempts to minimise the entropy, i.e.\ the spread of the probability mass, in the output distribution.
We use the normalised entropy, defined as
\begin{equation}
  \hat H(\mat x) = -\frac{1}{\log_2 D} \sum_{i=1}^D x_i \log_2 x_i.
\end{equation}
The normalisation ensures that the entropy is always bounded between $0$ and $1$, regardless of the number of outputs $D$ of the model.
Over a minibatch $B$, the entropy penalty is given as
\begin{multline}
  L_{\mathrm{H}}(\mat V; B) = \\ \frac{1}{2|B|} \sum_{i \in B} \left(\hat H(f(\mat x_i; \mat V)) + \hat H(f(\mat y_i; \mat V))\right)
\end{multline}

The entropy penalty implicitly encourages sparsity in $\mat W$, as the only way to avoid spreading the probability mass across several outputs is for each row of $\mat W$ to only contain a single element close to $1$.
This sparsity in turn makes it possible to convert the model into an exact partitioning of the input.
In summary, the complete loss over a minibatch $B$ is as follows:
\begin{equation}
  \label{eq:complete-loss}
  L(\mat V; B) = L_{\mathrm{JS}}(\mat V; B) + \lambda L_{\mathrm{H}}(\mat V; B)
\end{equation}
where $\lambda$ is a hyperparameter.

\begin{figure*}
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
  \begin{tikzpicture}
    \begin{groupplot}[
      group style={
	group size=2 by 1,
	horizontal sep=0.4cm,
	ylabels at=edge left,
	yticklabels at=edge left
      },
      ylabel=Divergence/entropy,
      %xmin=0,xmax=0.3,
      ymin=0,ymax=0.7,
      enlarge x limits=true,
      width=0.38\columnwidth,height=2cm]
   \nextgroupplot[title=English,xlabel=Penalty ($\lambda$),
      %legend style={column sep=10pt},
      legend entries={JS loss,Same-class loss,Different-class loss,Normalised entropy},
      legend columns=2,
      legend to name=grouplegend,legend cell align=left
      ]
   \addplot table[x=lambda,y=js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=same-js-v] {data/entropy_buckeye.txt};
   \addplot+[mark=triangle*,domain=0:0.3] table[x=lambda,y=diff-js-v] {data/entropy_buckeye.txt};
   \addplot table[x=lambda,y=entropy-v] {data/entropy_buckeye.txt};
   
   \nextgroupplot[title=Xitsonga,xlabel=Penalty ($\lambda$)]
   \addplot table[x=lambda,y=js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=same-js-v] {data/entropy_xitsonga.txt};
   \addplot+[mark=triangle*] table[x=lambda,y=diff-js-v] {data/entropy_xitsonga.txt};
   \addplot table[x=lambda,y=entropy-v] {data/entropy_xitsonga.txt};
  \end{groupplot}
  \node[yshift=-1.4cm] at ($(group c1r1.south)!.5!(group c2r1.south)$) {\ref{grouplegend}};
  \end{tikzpicture}
  
  \caption{\label{fig:entropy-penalty}}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
 \begin{tikzpicture}
   \pgfplotsset{set layers}
   \begin{axis}[
     xmin=0.8,xmax=4.2,
     ymax=0.17,
     axis y line*=left,
     xlabel=$\alpha$,
     ylabel=Silhouette,
     ylabel near ticks,
     yticklabel style={/pgf/number format/fixed},
     height=0.35\columnwidth,width=0.7\columnwidth,
     legend style={opacity=0.0}]%,
      %legend style={column sep=10pt},legend entries={Silhouette (English),Silhouette (Xitsonga)},legend cell align=left]
      \addplot table[x=alpha,y=sil-en] {data/silhouette.txt}; \label{sil1}
      \addlegendentry{Silhouette (English)}
   %\addplot+[mark=x] table[x=alpha,y=spread-en] {data/silhouette.txt};
   \addplot table[x=alpha,y=sil-ts] {data/silhouette.txt}; \label{sil2}
   \addlegendentry{Silhouette (Xitsonga)}
   %\addplot+[mark=x] table[x=alpha,y=spread-ts] {data/silhouette.txt};
   \end{axis}
   \begin{axis}[
     scale only axis,
     xmin=0.8,xmax=4.2,
     ymax=65,
     axis y line*=right,
     axis x line=none,
     ylabel=Outputs,
     ylabel near ticks,
     height=0.35\columnwidth,width=0.7\columnwidth,
     legend style={anchor=south,at={(0.5,1.02)}},
     legend columns=2]%,
      %legend style={column sep=10pt},legend entries={Silhouette (English),Silhouette (Xitsonga)},legend cell align=left]
   %\addplot+[mark=x] table[x=alpha,y=sil-en] {data/silhouette.txt};
   \addlegendimage{/pgfplots/refstyle=sil1}\addlegendentry{Silhouette (English)}
   \addlegendimage{/pgfplots/refstyle=sil2}\addlegendentry{Silhouette (Xitsonga)}
   \addplot+[dashed] table[x=alpha,y=spread-en] {data/silhouette.txt};
   \addlegendentry{Outputs (English)}
   %\addplot+[mark=x] table[x=alpha,y=sil-ts] {data/silhouette.txt};
   \addplot+[dashed] table[x=alpha,y=spread-ts] {data/silhouette.txt};
   \addlegendentry{Outputs (Xitsonga)}
   \end{axis}
 \end{tikzpicture}
 \caption{\label{fig:silhouette}}
\end{subfigure}

 \caption{\textbf{Left:} Normalised entropy of the output distribution and the combined, same-class and different-class Jensen-Shannon loss, averaged over the validation set, as a function of the entropy penalty $\lambda$ for the English and Xitsonga data sets.
 \textbf{Right:} Silhouette for different weightings $\alpha$ of the same-class and different-class losses.
 Also shown is a heuristically calculated estimate of the number of outputs used by the model for different $\alpha$.}
 %\vspace{-5mm}
\end{figure*}

\subsection{Binarising the model}
\label{sec:discrete}
As the resulting model is sparse, we can retrieve an exact partitioning by binarising the model.
We do this by for each row in $\mat W$ setting the largest element to $1$ and the remaining elements to $0$.
Using the binarised model as a base, we additionally experiment with binarising the output distribution by setting the largest output to 1 and the rest to 0; this can be thought of as taking the argmax of the output distribution.

% \subsection{Evaluation}
% \todo[inline]{maybe just refer to the ZeroSpeech/ABX papers instead of explaining how the evaluation works?}
% We evaluate the model on the minimal-pair ABX task \parencite{schatz2013evaluating}.
% In the task we are presented with three speech fragments A, B and X, where A and B form minimal pairs, i.e.\ they only differ by a single phoneme.
% The task is to decide which of either A or B belongs to the same category as X.
% This is done by aligning A and B with X using dynamic time warping (DTW) with respect to some underlying frame-based metric.
% The fragment closest to X according to the DTW score is chosen.
% The task takes two forms: within-speaker discriminability, where all fragments belong to the same speaker, and across-speaker discriminability, where A and B belong to one speaker while X belongs to another.
% The final score is the percentage of triples for which the wrong A or B was chosen.

%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
