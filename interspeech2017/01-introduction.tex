\section{Introduction}
\label{sec:introduction}
In the area of unsupervised learning in speech technology, researchers have, for many decades, attempted to reduce the amount of handcrafted information needed to build speech recognition and synthesis systems.
Many aspects of language acquisition have been covered, from finding sub-word units serving a similar role as pre-defined phonemes \parencite{LeeEtAl1988,SvendsenEtAl1989,BacchianiEtAl1996,huijbregts2011unsupervised, OGrady:2008up} to the discovery of recurrent patterns that may constitute word candidates \cite{Rasanen2011149, park2008unsupervised, Aimetti2010, StoutenEtAl2008phonepatterns, DriesenEtAl2009adaptivenon-negative, gs:VanhainenAndSalvi2012Interspeech, gs:VanhainenAndSalvi2014ICASSP}.
Recently, \citeauthor{versteegh2015zero} introduced the Zero Resource Speech Challenge \parencite{versteegh2015zero}, with the goal of standardising these endeavours.
Specifically, the first track, most relevant to this paper, involves finding speaker independent linguistic units from speech with no or weak supervision.
The discovery of linguistic units follows two main approaches in the literature that we will refer to as frame-based and term discovery-based. 
In the first case, acoustic models are inferred directly from the acoustic features \parencite{varadarajan2008unsupervised, lee2012nonparametric, siu2014unsupervised, chen2015parallel, zhang2010towards, versteegh2016zero, heck2016unsupervised, synnaeve2016temporal}.
The second approach is to first segment the speech into syllable- or word-like units, and afterwards break these units into smaller subword units \parencite{jansen2013weak, park2008unsupervised, jansen2011efficient, versteegh2015zero, jansen2011towards, jansen2013weak, synnaeve2014phonetics, thiolliere2015hybrid, versteegh2016zero, zeghidour2016deep, kamper2015unsupervised, renshaw2015comparison}.


\textbf{Frame-based approaches:}
\textcite{varadarajan2008unsupervised} first define a one-state HMM, and then iteratively split and merge states depending on the data and according to a heuristic.
The states of the final models (allophones), are then mapped into phonemes with the help of a separate model trained using labelled speech, making the method not fully unsupervised.
\textcite{lee2012nonparametric} use an infinite mixture model of three-state HMM-GMMs that performs segmentation and acoustic modelling jointly.
Inference of the model is done using Gibbs sampling.
A similar model but, without constraints on the topology of the HMMs was studied in \parencite{gs:VanhainenAndSalvi2014ICASSP}.
\textcite{siu2014unsupervised} first use a segmental GMM (SGMM) to generate a transcription of the data and then iteratively train a standard HMM to improve the transcriptions.
Note that the number of allowed states are here defined in advance.

Diverging from previous approaches which use temporal models, \textcite{chen2015parallel} perform standard clustering of speech frames using an infinite Gaussian mixture model.
After training, the speech frames are represented as posteriorgrams, which have been shown to be more speaker-invariant than other features such as MFCCs \parencite{zhang2010towards}.
Despite the simple approach, this turned out to be the overall best-performing model in the first track of the 2015 Zero Resource Speech Challenge \parencite{versteegh2016zero}.
\textcite{heck2016unsupervised} further improved on the model by performing clustering in two stages, with an intermediate supervised dimensionality reduction step using the clusters derived from the first clustering step as target classes.
In \parencite{synnaeve2016temporal} a siamese network \parencite{bromley1994signature} is used to create an embedding where speech frames close to each other are considered to belong to the same subword unit, while distant speech frames are said to differ.

\textbf{Term discovery-based approaches}
 use unsupervised term discovery (UTD) to extract word-like segments that can guide the discovery of more stable sub-word units compared to purely frame-based approaches.
The UTD is usually based on the segmental dynamic time warping (S-DTW) developed in \parencite{park2008unsupervised}.
In \parencite{jansen2011efficient} an approximate version is introduced that reduces the complexity from $O(n^2)$ to $O(n \log n)$ time.
This system also serves as the baseline for the second track of the Zero Resource Speech Challenge.
The information from UTD is used in \parencite{jansen2011towards} to train term-specific HMMs.
The states from each HMM are then clustered based on the similarity of their distributions, to form sub-word unit candidates.
A related approach is taken in \parencite{jansen2013weak}, where instead of HMM states, components from a GMM trained on speech frames are clustered based on co-occurence in pairs of fragments obtained from UTD.
A neural network referred to as the ABnet and based on siamese networks \parencite{bromley1994signature} is introduced in \parencite{synnaeve2014phonetics}.
The network takes a pair of speech frames as input, and adjusts its parameters so that the outputs are collinear if the inputs are known to correspond to the same subword unit, and orthogonal otherwise, using a cosine-based loss function.
\textcite{thiolliere2015hybrid} make use of this approach in the Zero Resource Speech Challenge, also incorporating UTD so as to make the whole process unsupervised, yielding competitive results \parencite{versteegh2016zero}.
\textcite{zeghidour2016deep} experiment with supplying the ABnet with scattering spectrum features instead of filter bank features, showing that with the right features, a shallow architecture may outperform a deep architecture, especially when the amount of available data is low.
\textcite{kamper2015unsupervised} use an autoencoder-like structure, where a neural network is trained to ``reconstruct'' a frame given another frame known to be of the same type.
\textcite{renshaw2015comparison} used this architecture in the Zero Resource Speech Challenge, albeit with a deeper decoder.

A limitation of term discovery-based approaches is that the UTD methods discussed here only discover a fraction of recurring patterns in the data, limiting the amount of available training data.

\textbf{This work}
takes inspiration from the two so far most successful approaches, namely the clustering approach of \parencite{chen2015parallel} and the siamese network approach of \parencite{thiolliere2015hybrid}.
We first cluster the data in an unsupervised manner using a GMM.
We then improve the resulting posteriorgrams using information from UTD by mapping speaker- or context-specific classes to broader classes with a linear siamese model.
This way we are able to take advantage of both the whole unlabelled data set, and the smaller set of fragments discovered by UTD.
While the approach of partitioning posteriorgrams is reminiscent of \parencite{jansen2013weak}, the major difference is that in place of direct clustering of classes, we are instead trying to maximise the similarity/dissimilarity between pairs of speech fragments, which only indirectly results in a partition of the classes.
Our linear model also has the advantage of being more interpretable than deep networks like that of \parencite{thiolliere2015hybrid}.


%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
