\section{Introduction}
\label{sec:introduction}
In the area of unsupervised learning in speech technology, researchers have, for many decades, attempted to reduce the amount of handcrafted information needed to build speech recognition and synthesis systems.
Many aspects of language acquisition have been covered, from finding sub-word units serving a similar role as pre-defined phonemes \parencite{LeeEtAl1988,SvendsenEtAl1989,BacchianiEtAl1996,huijbregts2011unsupervised, OGrady:2008up} to the discovery of recurrent patterns that may constitute word candidates \cite{Rasanen2011149, park2008unsupervised, Aimetti2010, StoutenEtAl2008phonepatterns, DriesenEtAl2009adaptivenon-negative, gs:VanhainenAndSalvi2012Interspeech, gs:VanhainenAndSalvi2014ICASSP}.
Recently, \citeauthor{versteegh2015zero} introduced the Zero Resource Speech Challenge \parencite{versteegh2015zero}, with the goal of standardising these endeavours.
Specifically, the first track, most relevant to this paper, involves finding speaker independent linguistic units from speech with no or weak supervision.
The discovery of linguistic units follows two main approaches in the literature that we will refer to as frame-based and term discovery-based. 
In the first case, acoustic models are inferred directly from the acoustic features \parencite{varadarajan2008unsupervised, lee2012nonparametric, siu2014unsupervised, chen2015parallel, zhang2010towards, versteegh2016zero, heck2016unsupervised, synnaeve2016temporal}.
The second approach is to first segment the speech into syllable- or word-like units, and afterwards break these units into smaller subword units \parencite{jansen2013weak, park2008unsupervised, jansen2011efficient, versteegh2015zero, jansen2011towards, jansen2013weak, synnaeve2014phonetics, thiolliere2015hybrid, versteegh2016zero, zeghidour2016deep, kamper2015unsupervised, renshaw2015comparison}.
%The motivation for taking this top-down approach is that sounds that in reality correspond to the same linguistic unit may seem very different when inspecting speech at specific time instances, especially when comparing different speakers.
%One approach that has proved itself successful in modelling linguistic units is to first discover recurring speech fragments, and then use these fragments as constraints to construct features where speech frames corresponding to the same sound are similar \parencite{synnaeve2014phonetics,thiolliere2015hybrid}.
%One motivation for taking this top-down approach is that sounds that in reality correspond to the same linguistic unit may seem very different when inspecting speech at specific time instances, especially when comparing different speakers; however, when viewed at a larger time scale, patterns from e.g.\ recurring words are easier to find \parencite{jansen2013weak}.

%Simpler approaches such as direct clustering of unlabelled speech has also been shown to perform well \parencite{chen2015parallel}.
%This work seeks to find whether it is possible to combine the two approaches, by first inferring a probabilistic model from unlabelled speech, and afterwards improving on this model using speech fragment information.
%This approach is similar to the one in \parencite{jansen2013weak}, where a universal background model in the form of a Gaussian mixture model is inferred and later partitioned, with the difference that the partitioning is done approximately using a linear siamese model inspired by \parencite{synnaeve2014phonetics}, taking advantage of both same-class and different-class fragment information.

\textbf{Frame-based approaches:} % I tried using \paragraph{} but it did not work
%Approaches to unsupervised acoustic modelling can broadly be divided into two categories: bottom-up methods that infer the acoustic model directly from the speech frames, and top-down methods that first segment the speech into syllable- or word-like units, and afterwards try break these units into smaller subword units.
%\subsubsection{Bottom-up approaches}
%As an individual speech frame only make up a fraction of a complete speech sound, it is natural to model and segment the speech using a model that can capture time dependencies, such as a hidden Markov model (HMM), rather than attempt to cluster the speech frames directly.
%One issue with this approach, however, is that the number of possible states (i.e.\ subword units) is unknown a priori.
\textcite{varadarajan2008unsupervised} first define a one-state HMM, and then iteratively split and merge states depending on the data and according to a heuristic.
%They finally interpret every state as an allophone.
%Training stops once the size of the HMM reaches a threshold.
%After training, each state in the HMM can be thought to correspond to some allophone (context-dependent variant realisation) of a phoneme.
The states of the final models (allophones), are then mapped into phonemes with the help of a separate model trained using labelled speech, making the method not fully unsupervised.
\textcite{lee2012nonparametric} use an infinite mixture model of tri-state HMM-GMMs that performs segmentation and acoustic modelling jointly.
%An infinite mixture model of tri-state HMM-GMMs modelling subword units is defined using the Dirichlet process, and latent variables representing segment boundaries are introduced.
%The data can be thought to be generated by repeatedly sampling an HMM to model a segment, sampling a path through the HMM, and for each state in the path sampling a feature vector from the corresponding GMM.
%The probability of transitioning from one unit to another is thus not modelled.
Inference of the model is done using Gibbs sampling.
A similar model but, without constraints on the topology of the HMMs was studied in \parencite{gs:VanhainenAndSalvi2014ICASSP}.
\textcite{siu2014unsupervised} first use a segmental GMM (SGMM) to generate a transcription of the data and then iteratively train a standard HMM to improve the transcriptions. % by, in turns, maximising the likelihood of the model parameters given the transcription, and the transcription given the model parameters.
% use an HMM of a more classic form to model the data.
%An initial transcription of the data in terms of state labels is first generated in an unsupervised manner using a segmental GMM (SGMM).
%The HMM and transcription are then iteratively updated, maximising the probability of the model parameters given the transcription, and the transcription given the model parameters.
Note that the number of allowed states are here defined in advance.
%$n$-gram statistics are then collected from the transcription and used for tasks such as unsupervised keyword discovery.

Diverging from previous approaches which use temporal models, \textcite{chen2015parallel} perform standard clustering of speech frames using an infinite Gaussian mixture model.
After training, the speech frames are represented as posteriorgrams, which have been shown to be more speaker-invariant than other features such as MFCCs \parencite{zhang2010towards}.
Despite the simple approach, this turned out to be the overall best-performing model in the first track of the 2015 Zero Resource Speech Challenge \parencite{versteegh2016zero}.
\textcite{heck2016unsupervised} further improved on the model by performing clustering in two stages, with an intermediate supervised dimensionality reduction step using the clusters derived from the first clustering step as target classes.
In \parencite{synnaeve2016temporal} a siamese network \parencite{bromley1994signature} is used to create an embedding where speech frames close to each other are considered to belong to the same subword unit, while distant speech frames are said to differ.
%A siamese network is a feedforward neural network that takes two inputs and adjusts its parameters to either maximise or minimise the similarity of the corresponding outputs \parencite{bromley1994signature}.

\textbf{Term discovery-based approaches} % I tried using \paragraph{} but it did not work
 use unsupervised term discovery (UTD) to extract word-like segments that can guide the discovery of more stable sub-word units compared to purely frame-based approaches.
%The rationale is that while at the frame level the same speech sound can seem quite different between different speakers or even different realisations of the sound by the same speaker, patterns over a longer duration of time are easier to identify; this idea is illustrated in \textcite{jansen2013weak}.
The UTD is usually based on the segmental dynamic time warping (S-DTW) developed in \parencite{park2008unsupervised}.
%This method, repeatedly performs DTW on two audio streams, each time changing the starting point of the DTW in both streams.
%This yields a set of alignments, from which the stretches of lowest average dissimilarity in each alignment can be extracted.
%Unfortunately, this approach is inherently $O(n^2)$ in time.
In \parencite{jansen2011efficient} an approximate version is introduced that reduces the complexity from $O(n^2)$ to $O(n \log n)$ time.
This system also serves as the baseline for the second track of the Zero Resource Speech Challenge. % \parencite{versteegh2015zero}.
The information from UTD is used in \parencite{jansen2011towards} to train term-specific HMMs.
% describes a method for finding subword units, assuming that clusters corresponding to words, each cluster containing multiple examples of that word in the form of audio, are given.
%For each word, an HMM is trained on all the corresponding examples, the number of states in the model being set to a number proportional to the average duration of the word.
The states from each HMM are then clustered based on the similarity of their distributions, to form sub-word unit candidates.
A related approach is taken in \parencite{jansen2013weak}, where instead of HMM states, components from a GMM trained on speech frames are clustered based on co-occurence in pairs of fragments obtained from UTD. %, with the assumption that each cluster will tend to correspond to some speaker- or context-dependent sub-word unit.
%The resulting speaker- and context-dependent clusters are then partitioned into broader classes based on co-occurrence in pairs of word-like segments obtained from UTD. %calculate how often clusters tend to co-occurr.
%The clusters are then partitioned so that clusters that co-occurr often are placed in the same partition.
A neural network referred to as the ABnet and based on siamese networks \parencite{bromley1994signature} is introduced in \parencite{synnaeve2014phonetics}.
%\textcite{synnaeve2014phonetics} introduce a neural network known referred to as the ABnet, based on siamese networks \parencite{bromley1994signature}.
The network takes a pair of speech frames as input, and adjusts its parameters so that the outputs are collinear if the inputs are known to correspond to the same subword unit, and orthogonal otherwise, using a cosine-based loss function.
\textcite{thiolliere2015hybrid} make use of this approach in the Zero Resource Speech Challenge, also incorporating UTD so as to make the whole process unsupervised, yielding competitive results \parencite{versteegh2016zero}.
\textcite{zeghidour2016deep} experiment with supplying the ABnet with scattering spectrum features instead of filter bank features, showing that with the right features, a shallow architecture may outperform a deep architecture, especially when the amount of available data is low.
\textcite{kamper2015unsupervised} use an autoencoder-like structure, where a neural network is trained to ``reconstruct'' a frame given another frame known to be of the same type.
\textcite{renshaw2015comparison} used this architecture in the Zero Resource Speech Challenge, albeit with a deeper decoder.

A limitation of term discovery-based approaches is that the UTD methods discussed here only discover a fraction of recurring patterns in the data, limiting the amount of available training data.

\textbf{This work} % I tried using \paragraph{} but it did not work
takes inspiration from the two so far most successful approaches, namely the clustering approach of \parencite{chen2015parallel} and the siamese network approach of \parencite{thiolliere2015hybrid}.
We first cluster the data in an unsupervised manner using a GMM.
We then improve the resulting posteriorgrams using information from UTD by mapping speaker- or context-specific classes to broader classes with a linear siamese model.
This way we are able to take advantage of both the whole unlabelled data set, and the smaller set of fragments discovered by UTD.
While the approach of partitioning posteriorgrams is reminiscent of \parencite{jansen2013weak}, the major difference is that in place of direct clustering of classes, we are instead trying to maximise the similarity/dissimilarity between pairs of speech fragments, which only indirectly results in a partitioning of the classes.
Our linear model also has the advantage of being more interpretable than deep networks like that of \parencite{thiolliere2015hybrid}.

%Many probabilistic models, such as Gaussian mixture models and hidden Markov models, have a concept of latent states or classes.
%We pose the problem of improving posteriorgrams from such a model as one of merging, or partitioning, these classes.
%By first training the model in a fully unsupervised manner, it learns classes that can generally be assumed to be highly speaker-specific.
%We can then use weak supervision to merge these classes, yielding representations that are more speaker invariant.

%A partitioning of classes can be viewed as a surjection from the original set of classes to a class set of lower cardinality, but finding this surjection is a discrete problem which is difficult to optimise for.
%However, a benefit of posteriorgrams is that the probability of an output class can be described as a simple sum of the probabilities of the classes that map to the class in question.
%This means that the surjection can be approximated using a continuous linear model which can be optimised through standard gradient descent.
%A linear model also has the added benefit of being more interpretable than deep networks such as that of \textcite{thiolliere2015hybrid}.
%While the approach of partitioning posteriorgrams is very reminiscent of \textcite{jansen2013weak}, the major difference is that in place of direct clustering of classes, we are instead trying to maximise the similarity/dissimilarity between pairs of speech fragments, which only indirectly results in a partitioning of the classes.


%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
