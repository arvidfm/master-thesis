\section{Introduction}
\label{sec:introduction}
%Automatic speech recognition (ASR) is generally framed as a supervised task, where both audio data and the corresponding transcription is available, and the problem is to develop a model that can mimic this mapping from speech to text.
%However, developing such data is expensive, both in terms of time and money, as it involves painstakingly transcribing many hours of speech and aligning the transcription in time by hand.
%As a result, there is a notable lack of high-quality data for speech recognition for a majority of languages around the world.
%An important question is thus whether it is possible to make use of untranscribed, or unlabelled, data to develop ASR for such low-resource languages.
%Unsupervised learning in this manner may also provide insight into the linguistic structure of languages, or the language acquisition of infants.

% in the parencite below I plan to collect citations from earlier times (there are many papers already from the '80s)
The area of unsupervised speech acquisition has been active for many decades \parencite{}, but only recently has gained attention due to the practical and economical burden of collecting high-quality data needed for supervised speech recognition.
\textcite{versteegh2015zero} recently introduced the Zero Resource Speech Challenge, which was developed with the goal of finding linguistic units (track 1) or longer recurring word-like fragments (track 2) in speech.
Models are to be trained using only speech data, voice activity information, and speaker identity information.
%In the spirit of the Zero Resource Speech Challenge, this thesis will follow the first track of the challenge, using the same training data and evaluation procedure.

One approach that has proved itself successful in modelling linguistic units is to first discover recurring speech fragments, and then use these fragments as constraints to construct features where speech frames corresponding to the same sound are similar \parencite{synnaeve2014phonetics,thiolliere2015hybrid}.
One motivation for taking this top-down approach is that sounds that in reality correspond to the same linguistic unit may seem very different when inspecting speech at specific time instances, especially when comparing different speakers; however, when viewed at a larger time scale, patterns from e.g.\ recurring words are easier to find \parencite{jansen2013weak}.

Simpler approaches such as direct clustering of unlabelled speech has also been shown to perform well \parencite{chen2015parallel}.
This work seeks to find whether it is possible to combine the two approaches, by first inferring a probabilistic model from unlabelled speech, and afterwards improving on this model using speech fragment information.
This approach is similar to the one in \parencite{jansen2013weak}, where a universal background model in the form of a Gaussian mixture model is inferred and later partitioned, with the difference that the partitioning is done approximately using a linear siamese model inspired by \parencite{synnaeve2014phonetics}, taking advantage of both same-class and different-class fragment information.

\subsection{Related work}

Approaches to unsupervised acoustic modelling can broadly be divided into two categories: bottom-up methods that infer the acoustic model directly from the speech frames, and top-down methods that first segment the speech into syllable- or word-like units, and afterwards try break these units into smaller subword units.

\subsubsection{Bottom-up approaches}

As an individual speech frame only make up a fraction of a complete speech sound, it is natural to model and segment the speech using a model that can capture time dependencies, such as a hidden Markov model (HMM), rather than attempt to cluster the speech frames directly.
One issue with this approach, however, is that the number of possible states (i.e.\ subword units) is unknown a priori.

\textcite{varadarajan2008unsupervised} tackle this problem by first defining a one-state HMM, and then iteratively splitting and merging states as needed to account for the data according to a heuristic.
Training stops once the size of the HMM reaches a threshold.
After training, each state in the HMM can be thought to correspond to some allophone (context-dependent variant realisation) of a phoneme.
It should be noted, however, that in order to interpret a given state sequence as a single phoneme, \citeauthor{varadarajan2008unsupervised} train a separate model using labelled speech to perform this mapping.
The method is thus not fully unsupervised.

\textcite{lee2012nonparametric} take a fully probabilistic approach, defining a model that jointly performs segmentation and acoustic modelling.
An infinite mixture model of tri-state HMM-GMMs modelling subword units is defined using the Dirichlet process, and latent variables representing segment boundaries are introduced.
The data can be thought to be generated by repeatedly sampling an HMM to model a segment, sampling a path through the HMM, and for each state in the path sampling a feature vector from the corresponding GMM.
The probability of transitioning from one unit to another is thus not modelled.
Inference of the model is done using Gibbs sampling.

\textcite{siu2014unsupervised} use an HMM of a more classic form to model the data.
An initial transcription of the data in terms of state labels is first generated in an unsupervised manner using a segmental GMM (SGMM).
The HMM and transcription are then iteratively updated, maximising the probability of the model parameters given the transcription, and the transcription given the model parameters.
Note that the number of allowed states are here defined in advance.
$n$-gram statistics are then collected from the transcription and used for tasks such as unsupervised keyword discovery.

Diverging from previous approaches using temporal models, \textcite{chen2015parallel} perform standard clustering of speech frames using an infinite Gaussian mixture model.
After training, the speech frames are represented as posteriorgrams, which have been shown to be more speaker-invariant than other features such as MFCCs \parencite{zhang2010towards}.
Despite the simple approach, this turned out to be the overall best-performing model in the first track of the 2015 Zero Resource Speech Challenge \parencite{versteegh2016zero}.
\textcite{heck2016unsupervised} later further improved on the model by performing clustering in two stages, with an intermediate supervised dimensionality reduction step using the clusters derived from the first clustering step as target classes.

\textcite{synnaeve2016temporal} use a siamese network to create an embedding where speech frames close to each other are considered to belong to the same subword unit, while distant speech frames are said to differ.
A siamese network is a feedforward neural network that takes two inputs and adjusts its parameters to either maximise or minimise the similarity of the corresponding outputs \parencite{bromley1994signature}.

\subsubsection{Top-down approaches}
\todo[inline]{remove information already included in the introduction}

Top-down approaches start by first finding pairs of longer word-like segments using unsupervised term discovery (UTD).
This information provides constraints that can be use to find speech frame representations that are more stable within a given subword unit.
The rationale is that while at the frame level the same speech sound can seem quite different between different speakers or even different realisations of the sound by the same speaker, patterns over a longer duration of time are easier to identify; this idea is illustrated in \textcite{jansen2013weak}.

The UTD systems used in this context are generally based on the segmental dynamic time warping (S-DTW) developed by \textcite{park2008unsupervised}.
S-DTW works by repeatedly performing DTW on two audio streams while constraining the maximum amount of warping allowed, each time changing the starting point of the DTW in both streams.
This yields a set of alignments, from which the stretches of lowest average dissimilarity in each alignment can be extracted.
Unfortunately, this approach is inherently $O(n^2)$ in time.
To remedy this, \textcite{jansen2011efficient} introduced an approximate version that uses binary approximations of the feature vectors to perform the calculations in $O(n \log n)$ time using sparse similarity matrices; this system also serves as the baseline for the second track of the Zero Resource Speech Challenge \parencite{versteegh2015zero}.

\textcite{jansen2011towards} describe a method for finding subword units, assuming that clusters corresponding to words, each cluster containing multiple examples of that word in the form of audio, are given.
For each word, an HMM is trained on all the corresponding examples, the number of states in the model being set to a number proportional to the average duration of the word.
The states from each HMM are then collected and clustered based on the similarity of their distributions, forming clusters that hopefully correspond to subword units.

\textcite{jansen2013weak} take somewhat of an inverse approach, starting by clustering the whole data on a frame level, with the assumption that each cluster will tend to correspond to some speaker- or context-dependent subword unit.
They then look at pairs of word-like segments known to be of the same type and calculate how often clusters tend to co-occurr.
The clusters are then partitioned so that clusters that co-occurr often are placed in the same partition.

\textcite{synnaeve2014phonetics} introduce a neural network known referred to as the ABnet, based on siamese networks \parencite{bromley1994signature}.
The network takes a pair of speech frames as input, and adjusts its parameters so that the outputs are collinear if the inputs are known to correspond to the same subword unit, and orthogonal otherwise, using a cosine-based loss function.
\textcite{thiolliere2015hybrid} made use of this approach in the Zero Resource Speech Challenge, also incorporating unsupervised term discovery so as to make the whole process unsupervised, yielding competitive results \parencite{versteegh2016zero}.
\textcite{zeghidour2016deep} experiment with supplying the ABnet with scattering spectrum features instead of filter bank features, showing that with the right features, a shallow architecture may outperform a deep architecture, especially when the amount of available data is low.

\textcite{kamper2015unsupervised} use an autoencoder-like structure, where a neural network is trained to ``reconstruct'' a frame given another frame known to be of the same type.
\textcite{renshaw2015comparison} used this architecture in the Zero Resource Speech Challenge, albeit with a deeper decoder.

\subsection{This thesis}
\todo[inline]{make description more application agnostic?}

Two of the most successful approaches so far are the clustering approach of \textcite{chen2015parallel} and the siamese network approach of \textcite{thiolliere2015hybrid}.
We pose the question of whether it is possible to combine the two approaches by first clustering the data in an unsupervised manner using a probabilistic model, and then improving the resulting posteriorgrams using speech fragment information.
This way we are able to take advantage of both the whole unlabelled data set, and the smaller set of discovered fragments.

Many probabilistic models, such as Gaussian mixture models and hidden Markov models, have a concept of latent states or classes.
We pose the problem of improving posteriorgrams from such a model as one of merging, or partitioning, these classes.
By first training the model in a fully unsupervised manner, it learns classes that can generally be assumed to be highly speaker-specific.
We can then use weak supervision to merge these classes, yielding representations that are more speaker invariant.

A partitioning of classes can be viewed as a surjection from the original set of classes to a class set of lower cardinality, but finding this surjection is a discrete problem which is difficult to optimise for.
However, a benefit of posteriorgrams is that the probability of an output class can be described as a simple sum of the probabilities of the classes that map to the class in question.
This means that the surjection can be approximated using a continuous linear model which can be optimised through standard gradient descent.
A linear model also has the added benefit of being more interpretable than deep networks such as that of \textcite{thiolliere2015hybrid}.
While the approach of partitioning posteriorgrams is very reminiscent of \textcite{jansen2013weak}, the major difference is that in place of direct clustering of classes, we are instead trying to maximise the similarity/dissimilarity between pairs of speech fragments, which only indirectly results in a partitioning of the classes.


%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
